---

title: "[MIT6.034] 17.서포트 벡터 머신과 커널 기법"
date: 2025-06-05 08:30:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, SVM, Kernel-Trick, Optimization, Convexity, Vapnik]
math: true
---

**MIT AI 강의 17 - 서포트 벡터 머신(SVM)과 커널을 통한 공간 변환 학습**

---

## **개요**

* 이번 강의는 **결정 경계(decision boundaries)** 를 효율적으로 정의하는 방법
* 기존 기법(ID Tree, Neural Net, k-NN 등)을 뛰어넘는(?? 예전 강의기준인듯?) **서포트 벡터 머신(SVM)** 을 학습
* 커널 기법을 통한 **비선형 분리 문제 해결**

---

## **SVM 개념: 최대 마진 선형 분리**

### **목표**

* 양성(positive)과 음성(negative) 샘플을 **가장 넓은 도로(widest street)** 로 구분하는 선형 결정경계 도출
* 다시 말해서, 선형 분류기 $w \cdot x + b = 0$를 이용하며, 두 클래스를 가장 넓게 분리하는 경계를 찾는 것임

### **핵심 조건**

* 결정 경계는 $w \cdot x + b = 0$
* 샘플은 다음을 만족해야 함:

  * $$y_i (w \cdot x_i + b) \geq 1$$
  * $$y_i \in {+1, -1}$$
  * $y_i$ 는 각 샘플의 정답 레이블: $+1$ 또는 $-1$, $x_i$는 각 샘플 벡터 

  > - $w$는 결정 경계에 수직인 벡터이며, 마진이다.
  > - $y_i (w \cdot x_i + b) > 0$ 이 조건만으로도 분류는 가능하지만, 마진(안전 거리)에 대한 제약이 없다.
  > - SVM은 마진을 고려한다. 따라서 분류 조건에 다음과 같은 제약을 건다:
  > > - $y_i (w \cdot x_i + b) \geq 1$
  > > - 이 조건을 부과하면:
  > >   - \( y_i = 1 \) → \( w \cdot x_i + b \geq 1 \)
  > >   - \( y_i = -1 \) → \( w \cdot x_i + b \leq -1 \)
  > > - → 즉, **결정 경계로부터 각 클래스가 적어도 1의 거리만큼 떨어져 있어야 함**




#### **마진이란 무엇인가?**

* 점과 초평면 사이 거리 공식
* 어떤 점 $x_0$이 있고, 초평면 $w \cdot x + b = 0$이 있을 때, 두 객체 사이의 거리:
  $$
  \text{거리} = \frac{|w \cdot x_0 + b|}{\|w\|}
  $$


> * 두 벡터 $\vec{a}, \vec{b}$가 수직일 조건: $\vec{a} \cdot \vec{b} = \|\vec{a}\| \cdot \|\vec{b}\| \cdot \cos\theta$
> * → $\theta = 90^\circ$ 이면, $\cos\theta = 0$ ⇒ $\vec{a} \cdot \vec{b} = 0 \Rightarrow \vec{a} \perp \vec{b}$
> * 즉, **내적(inner product)이 0이면 두 벡터는 수직이다.**

> **직선/평면의 식과 법선 벡터**
> * 2D 직선 예시
>   * 직선 식: $2x + 3y + 1 = 0$
>   * → 벡터로 표현: $\begin{bmatrix}2 & 3\end{bmatrix}\cdot\begin{bmatrix}x \\ y\end{bmatrix}+ 1 = 0$
>   * → 여기서 $w = [2, 3]$ 는 **직선에 수직인 방향**, 즉 **법선 벡터**
> * 왜 $w$ 가 법선인가?
>   * 직선 위의 두 점 $x_1, x_2$ 에 대해: $x_1 - x_2$는 직선 방향 벡터 $w \cdot (x_1 - x_2) = 0$  
>   * → $w$ 는 직선의 방향과 수직 → 따라서 $w$ 는 **법선 벡터**
> * n차원에서도 동일
>   * 평면의 일반식: $w \cdot x + b = 0$
>   * → 여기서 $w$는 **평면과 수직인 벡터**, 즉 **법선 벡터**
> * 이유:
>   * 평면 위의 임의의 두 점 $$x_1, x_2$$를 고르면 $x_1 - x_2$는 평면을 따라 움직이는 벡터
>   * 이 벡터는 $w$와 항상 수직:
>   * $w \cdot (x_1 - x_2) = 0$ ⇒ $w$는 **평면에 수직** ⇒ $w$는 **법선(normal vector)**

> **정사영(Projection) 개념**
> * 정의: 벡터 $\vec{v}$를 어떤 벡터 $\vec{u}$ 방향으로 **그림자처럼** 내려 찍는 것
> * 정사영 공식: $\text{proj}_u(v) = \frac{v \cdot u}{\|u\|^2} u$ → 방향은 $u$와 같고, 크기는 $v$에서 $u$로의 겹친 정도

> **점과 평면 사이 거리 공식**
> * 점 $x_0$과 평면 $w \cdot x + b = 0$ 사이 거리:
> * $\text{거리} = \frac{|w \cdot x_0 + b|}{\|w\|}$
> * 유도 과정 핵심:
>   * 평면 위의 점 $x'$를 기준으로 벡터 $x_0 - x'$를 만든 뒤 그 벡터를 **법선 방향 $w$로 정사영** 한 길이 사용


* SVM은 결정 경계 양쪽에 다음과 같은 **마진 경계(Margin Boundaries)** 를 둠:
  - $w \cdot x + b = +1$ 
  - $w \cdot x + b = -1$ 
* 이 두 경계 사이에 **어떤 데이터도 침범하지 않아야 함**  
* 이 두 평면(경계선) 사이의 거리는 다음과 같음:
* $\text{Margin} = \frac{|(+1) - (-1)|}{\|w\|} = \frac{2}{\|w\|}$

---

## **목적 함수와 라그랑지 승수법**

### **목적**

* **마진 최대화 ⇨ $|w|$ 최소화 ⇨ $\frac{1}{2} |w|^2$ 최소화**
  > ※ 미분을 깔끔하게 하기 위해 $\frac{1}{2}$를 붙임

* 제약조건:
* $y_i(w \cdot x_i + b) \geq 1 \quad \text{for all } i$
  > * 음의 클래스의 경우 -1을 곱하면 부등호가 반대로 되니 해당 경우도 만족함. 해당 조건은 양의 클래스, 음의 클래스 두 선 사이 **마진** 에 데이터가 없음.
  > * 왜 +1, -1로 두는가?
  >   * +3, -2 이렇게 두면 위의 식처럼 한줄로 제약을 표현하기가 힘듬.
  >   * 어차피 $w, b$로 스케일은 자유도 있고, +1, -1로 표현하는것이 정규화에 편함.


### **라그랑지언(Lagrangian)**

제약 조건이 있는 최적화 문제이므로, **라그랑지 승수법** 사용:  

$$
L = \frac{1}{2}|w|^2 - \sum_i \alpha_i [y_i(w \cdot x_i + b) - 1]
$$

- $\alpha_i \geq 0$는 각 샘플의 라그랑지 승수
  > - 라그랑주 승수란?
  >   - 제약 조건이 있는 최적화 문제를 풀기 위해, 각 제약마다 **하나의 새로운 변수(alpha)** 를 붙이는 방법이다
  >   - 이렇게 하면 제약 조건을 위반하지 않으면서 최적화를 할 수 있다
- 이 식을 최소화하면서, 모든 제약 조건이 만족되도록 한다
- 최소화하기 위해서 → $w$, $b$ 에 대해 편미분 = 0 하는 조건을 찾는다. (페널티인 $\alpha$ 는 최대값찾기.)
  > - 기울기가 0인 지점에서 극값(최솟값, 최댓값 또는 saddle point)이 발생함.
  > - **stationary point (정지점)** 에서 함수의 값이 더 이상 줄거나 늘지 않음 → 최솟값 후보
  > - SVM은 convex 문제이며 convex 에서는 극값은 전역 최소값이다.
  > - 마진은 볼록함수, 제약조건은 선형이므로 문제 없음.

  > **볼록 함수(convex function)의 정의**
  > - 어떤 함수 $f(w)$가 **볼록(convex)** 하다는 것은 다음을 만족하는 것:
  > - $f(\lambda w_1 + (1 - \lambda) w_2) \leq \lambda f(w_1) + (1 - \lambda) f(w_2)$ ($0 \leq \lambda \leq 1$)
  > - 즉, 두 점을 이은 **직선이 함수 그래프 위에 있음**
  > - ⇒ 아래로 휘어진 형태 ⇒ **전역 최소값이 존재**

  > SVM은 다음을 최소화함:
  > - $\frac{1}{2} |w|^2 = \frac{1}{2} \sum_{i=1}^d w_i^2$
  > - 해당 식은 각 변수 $w_i$의 제곱합이자 2차 함수의 합이다.
  > - 2차항은 볼록함수, 볼록함수의 합은 볼록함수이므로 **SVM은 볼록함수이다.**
  > - 헤시안(Hessian) 행렬로 확인할 수도 있음. SVM 마진의 헤시안은 항등행렬 $I$ 이며 양의 정부호로 볼록함. ($\nabla^2 f(w) = I$)



### **파생 조건들**
Primal 문제 해결
* $w$, $b$ 에 대한 편미분 수행
* $\frac{\partial L}{\partial w} = w - \sum_i \alpha_i y_i x_i = 0 \Rightarrow w = \sum_i \alpha_i y_i x_i$

  * 결정 경계의 방향벡터 $w$는 **서포트 벡터들**의 선형 조합으로 표현됨.
  * 즉 훈련데이터 $x_i$ 를 조합해서 만들어졌다는 뜻.
  * 그 중에서도 오직 $\alpha_i$가 0이 아닌 샘플들만 영향을 줌.
  * SVM은 마진 가장자리에 서 있는 경계 대표자들만 보고 직선을 결정하고자 함.
  * 나머진 경계에 영향을 주지 않음(왜냐하면 이미 마진 바깥에 잘 분리되어 있어서)
  * 그런데 **서포트 벡터들**가 어떤 샘플인지 모르는데 $\alpha_i$ 을 즉, 가장자리를 어떻게 정하지?
      * **SVM 최적화(dual Problem)** 의 본질
      * 가장자리, $\alpha_i$ 를 구하자.


* $\frac{\partial L}{\partial b} = \sum_i \alpha_i y_i = 0$
  * b가 최소화될 때의 조건임.  

  
한계점
  * 어떤 샘플이 중요한지 **사전에 알 수 없다**
  * 모든 샘플을 다 넣고 계산해야 한다 → 비효율적이다
  * 현실의 데이터는 직선으로는 분리되지 않는 경우가 많다. (예: 원형 데이터, 나선형 분포 등)
    * **커널 기법(Kernel Trick)** 이 필요하다.
        *  데이터를 더 높은 차원으로 옮겨서, 원래는 복잡했던 분포를 **직선으로 분리 가능하게** 만든다
        *  이때 중요한 것은 **직접 변환하지 않고**, **두 점 사이의 유사도만 계산하는 함수**를 쓰는 것이다 
     *  하지만 Primal 방식은 $w$와 $x$의 곱을 직접 계산하기 때문에, 고차원 커널을 도입하려면 수식이 복잡해진다. → 커널 기법과 **직접적인 궁합이 좋지 않다**

문제 해결
* 듀얼 폼이란?
  - 원래 문제를 **다른 방식으로 표현한 최적화 문제**
  - 변수도 달라지고, 관점도 달라진다
  - **Primal은 "직선을 직접 찾는 문제"**  
  - **Dual은 "경계를 만들기 위해 어떤 샘플이 중요한지를 찾는 문제"**


### **듀얼 폼을 이용한 알파(α) 찾기**

SVM은 다음 최적화 문제(Dual form)를 풉니다:

$$
\max_\alpha \left( \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) \right)
$$

어떻게 해당식이 나오게 되었는가?
* 원래의 식은 $\frac{1}{2} |w|^2$의 최소값을 구하며, Penalty로 $\alpha$를 부여하고 제약사항을 둠.
* $\alpha$ 는 커질수록 penaltiy로 값을 최소화하니 최대값을 찾아야함.
* 원래의 식의 $w, b$ 을 위의 편미분 식을 이용하여 대입하여 $\alpha$ 로 표현하면 됨.


**제약 조건**:

- $$\alpha_i \geq 0$$
- $$\sum_i \alpha_i y_i = 0$$



* 듀얼 폼을 만들면 어떤 일이 일어나는가?
  * **알파(alpha)** : **샘플별 중요도** 
  * 결정 경계 w 자체는 사라짐.
  * 식에는 오직 **샘플 간 내적(xi · xj)** 만 등장 → 커널 기법을 쉽게 적용할 수 있다

## **왜 듀얼로 풀어야 하는가?**

* 서포트 벡터를 자동으로 찾는다
  - 최적화 후 알파가 0보다 큰 샘플만 결정 경계에 기여한다
  - 알파 = 0인 샘플은 **무시해도 된다**
  - 어떤 샘플이 중요한지 **자동으로 판별된다**
    - 최적화
      > **2차 계획법(QP, Quadratic Programming)** 이라는 수학적 최적화 기법으로 Solve. Python에서는 cvxopt, scikit-learn, libsvm 을 사용함.

* 계산 효율성이 높아진다
  - 대부분의 알파는 0이기 때문에 계산에 참여할 샘플 수가 줄어든다
  - 고차원에서도 실제 계산량은 크게 증가하지 않는다

* 커널 함수 적용이 간단하다
  - 내적$(x_i · x_j)$을 커널 함수 $K(x_i, x_j)$로 바꾸면 끝
  - 계산은 원래 차원에서 하고, 결과는 고차원 효과를 누릴 수 있다
  > **커널은 왜 필요한가?**
  > - 직선(혹은 초평면)으로 데이터를 나눌 수 없는 경우가 많다. 예를 들어 동그란 점은 중심에 몰려 있고 세모난 점은 바깥에 둘러 있는 경우 공간에서는 절대로 직선으로 구분할 수 없습니다. 
  > - 이걸 수학적으로는 어떤 함수 φ(x) 를 써서, 데이터를 고차원 특징 공간으로 옮긴다. 그런데 직접 고차원으로 옮기면 계산량이 폭발하니까, 커널 함수(Kernel Function) 라는 우회 전략을 쓴다.

  - **커널 트릭이란?**
    - SVM의 듀얼 문제는 α를 구할 때 항상 $(x_i · x_j)$ 같은 내적만 사용한다.
    - "$x_i · x_j$" → "$K(x_i, x_j)$"
    - 이때 K는 마치 고차원으로 올린 것처럼 행동하는 함수인데, 실제로 고차원 계산은 하지 않아도 되는 게 핵심이다.
    - 대표적인 커널
      - 선형 커널: $x_i · x_j$
      - 다항 커널: $(x_i · x_j + c)^d$
      - RBF 커널:	exp$(– ‖x_i – x_j‖² / 2σ²)$ → 무한 차원 공간에서 작동


---

### **커널 트릭의 직관적 비유**

#### **문제 예시**

예를 들어서, 아래처럼 생긴 데이터가 있을 수 있다.

* 가운데는 클래스 A (●)
* 바깥 원형 띠는 클래스 B (○)

```
  ○ ○ ○ ○ ○  
○         ○  
○   ● ●   ○  
○         ○  
  ○ ○ ○ ○ ○  
```

중심과 바깥이 뒤섞여 있으니 **직선으로 나누는 건 불가능**하다.

#### **해결 아이디어: 차원을 늘린다**


* 새로운 z축(3번째 차원)을 다음처럼 정의합니다:
  - $z = x₁² + x₂²$
  - 즉, 각 점의 원점으로부터의 거리 제곱
* 결과:
  * 중심에 있던 ● 들은 x₁² + x₂² 가 작음 → z 값 작음 → **낮은 곳**
  * 바깥에 있던 ○ 들은 거리 멀음 → z 값 큼 → **높은 곳**

→ 전체 데이터가 **밥그릇처럼 생긴 포물면(paraboloid)** 위로 퍼지게 됩니다.

```
  위에서 본 모양: ●는 중심, ○는 바깥

  옆에서 본 3차원 모양: 바깥(○)이 더 높이 떠 있음

      z
      ↑
    ○   ○   ○    ← 높은 z 값
       ● ●       ← 낮은 z 값
      ↓
```

그럼 이제 **수평으로 평면 하나만 그으면**, ●와 ○를 잘 나눌수 있다.

### **커널 트릭의 핵심**

* 직선으로 못 나누는 데이터를, **공간을 왜곡하거나 차원을 늘려서** 직선으로 자르기 쉽게 만드는 것.
* 하지만 실제로 이걸 계산하려면 x₁², x₂², x₁x₂ 같은 항들을 계산해야 하니까 복잡하다.
* → 그래서 등장한 게 커널 함수이다. **내적만 바꿔치기(Kernel Trick)** 하면, 실제로는 변환을 안 해도 **마치 고차원에서 계산한 것 같은 효과**를 냅니다.

내적만 바꿔치기라는 말이 뭐지??


> **라그랑주도 똑같이 적용하면 안되나? 왜 듀얼만 QP로 푸는 걸까?**
> - 문제점:
>   - 라그랑주 prime 문제의 경우 변수, **w (벡터), b (스칼라)**의 차원이 고정되어 있음
>   - 제약조건은 각 샘플마다 한개씩으로 많은 선형 제약이 생긴다.
>   - 비선형 커널을 쓰면 **w 자체의 정의가 불가능하다**
>     - x 의 차원을 올리면 w도 차원을 올려야하는데 불가능함.
>


> **처음엔 누가 서포트 벡터인지 모름**
> → 그래서 모든 샘플에 $\alpha$를 둠  
> → 최적화를 통해 **필요한 샘플만 $\alpha > 0$** 로 결정됨
> → $\alpha > 0$ 인 샘플들이 서포트 벡터가 된다.

> **φ(x)를 직접 만들 필요는 없지만, φ(x)에 대응되는 커널 함수 K(x, z)는 알아야 하지 않나?**
> - 실전에서는 보통 몇 가지 대표 커널 함수들 중에서 골라 쓴다.
> - 다항식의 경우 커널은 괄호 전개와 내적 계산의 합성 공식일 뿐임.

> **차원을 계속 올리면 정말 항상 선형으로 분리 가능한가?**
> **Cover's Theorem (커버의 정리)**
> - "선형적으로 분리되지 않는 문제도, 비선형적으로 매핑된 고차원 공간에서는 선형 분리될 가능성이 높아진다."
> - possibility 일뿐이지 항상 된다는 말은 아님.
>   - 두 클래스가 서로 너무 얽히거나, 확률적으로 섞여 있는 경우 무한차원이라도 유사도 개념으로만 동작하기때문에 분리 경계 형성이 어려울 수 있음.

> **RBF 커널이 효과적인 이유**
> - **1. 지역성 (Locality)**
>   - RBF 커널은 두 점 사이의 거리 $\|x - z\|$ 에 따라 유사도를 결정한다.
>   - 가까우면 $K(x, z) \approx 1$, 멀면 $K(x, z) \approx 0$
>   - 이로 인해 **각 서포트 벡터 주변에만 영향을 미치는 “버블”** 이 생긴다.
>   - 복잡한 데이터 분포에서도 국소적으로 작용하며 분리 가능성을 높인다.
> - **2. 무한차원 특징 공간 → 유연한 분류 경계**
>   - RBF 커널은 무한차원 특징 공간의 내적을 계산하는 커널이다.
>   - 따라서 모든 차수의 다항 항들을 **암묵적으로 포함**한다.
>   - → **곡선, 원형, 복잡한 비선형 경계까지 모두 분리 가능**
> - **3. 부드럽고 자연스러운 영향력**
>   - RBF 커널은 **거리의 제곱에 따라 지수적으로 감소**한다:
>   - $K(x, z) = \exp\left(-\frac{\|x - z\|^2}{2\sigma^2}\right)$
>   - 경계 주변에서도 유사도가 부드럽게 변하여 **급격한 튐 없이 자연스러운 경계 생성**
>   - 과도한 민감도 없이, **일반화 성능을 유지**할 수 있음
> - **4. 왜 하필 이 거리 수식인가?**
>   - 위 식은 **정규분포(Gaussian)** 의 핵심 구조에서 유래:
>   - $\mathcal{N}(x \mid \mu, \sigma^2) \propto \exp\left( -\frac{\|x - \mu\|^2}{2\sigma^2} \right)$
>   - RBF 커널은 각 훈련 샘플을 **가우시안 중심**처럼 생각하고, 테스트 점이 그 중심과 가까울수록 높은 반응을 보인다.
>   - **확률적 의미 + 유사도 기반 분류**를 동시에 만족
> - **5. σ (시그마)의 역할**
>   - 위 식은 **정규분포(Gaussian)** 의 핵심 구조에서 유래:
>   - $\sigma$ 는 버블의 크기, 즉 **영향력 범위**를 결정한다.
>   - 작을수록 → 국소적, 날카로운 경계 → 과적합 위험
>   - 클수록 → 매끄러운 경계, 덜 민감 → 언더핏 가능
>   - → σ 를 조정해 **복잡도와 일반화 사이를 조율**할 수 있다.



---

## **목적 함수**

$$
L = \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)
$$

> 이 목적 함수는 **샘플 간 내적(dot product)** 만을 기반으로 구성됨 → 커널 기법의 기반
> 목적 함수를 통하여 $\alpha$ 와 b를 구하고, 최종 결정함수에서 새로운 입력값 $u$에 대한 클래스 구분이 가능하도록 한다.

### **최종 결정 함수**

$$
\text{sign} \left(\sum_i \alpha_i y_i (x_i \cdot u) + b \right)
$$

* 입력데이터 $u$에 대한 클래스 (+1, -1) 분류..
* 
  | 기호       | 의미                               | 역할                                     |
  | ---------- | ---------------------------------- | ---------------------------------------- |
  | $x_i$      | 훈련 샘플                          | SVM이 학습에 사용한 데이터               |
  | $y_i$      | 훈련 샘플의 정답 레이블 (±1)       | 각 $x_i$ 가 어떤 클래스에 속했는지       |
  | $\alpha_i$ | SVM이 학습을 통해 찾은 계수        | 서포트 벡터는 $\alpha_i > 0$, 나머지는 0 |
  | $b$        | 절편 (bias)                        | 경계 평면의 위치 조정                    |
  | $u$        | **새로운 입력 데이터 (예측 대상)** | → 우리가 분류하고 싶은 **테스트 데이터** |



---


## **SVM의 장점 vs 신경망**

| 항목        | SVM                         | Neural Net                 |
| ----------- | --------------------------- | -------------------------- |
| 최적화 구조 | **Convex** (전역 최적 보장) | 비볼록 (Local minima 위험) |
| 계산 대상   | 벡터 간 내적                | 계층별 비선형 조합         |
| 커널 확장   | 가능                        | 제한적                     |

---

## **역사와 통찰**

* Vapnik의 박사논문(1960s, 소련): SVM 아이디어 존재
* **1990s**: 미국 이민 후, Bell Labs에서 손글씨 인식 문제에 SVM 적용
* NIPS에 제출한 3편 논문 모두 **거절**, 이후 커널 기법 발전
* 커널의 위력 발견: **단순 선형 문제에 고차원 특징 부여 가능**
* Vapnik은 이후 **기계학습의 거장**으로 자리잡음

> 💡 교훈: 위대한 아이디어는 종종 **수십년 뒤** 조명받는다

---

## **정리**

* SVM은 **마진을 최대화**하여 일반화 성능을 확보하는 분류 기법
* 수학적으로는 **볼록 최적화(convex optimization)**
* **커널 트릭을 이용해** 비선형 문제까지 일반화
* 최종 분류/결정은 **샘플과의 내적 정보**만을 기반으로 이루어짐

> 💬 핵심 메시지: **문제를 해결 못할 때는 시야를 바꾸라 (change perspective)**
