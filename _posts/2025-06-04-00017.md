---

title: "[MIT6.034] 17.서포트 벡터 머신과 커널 기법"
date: 2025-06-05 08:30:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, SVM, Kernel-Trick, Optimization, Convexity, Vapnik]
math: true
---

**🧠 MIT AI 강의 17 - 서포트 벡터 머신(SVM)과 커널을 통한 공간 변환 학습**

---

## 🚀 개요

* 이번 강의는 **결정 경계(decision boundaries)**를 효율적으로 정의하는 방법
* 기존 기법(ID Tree, Neural Net, k-NN 등)을 뛰어넘는 **서포트 벡터 머신(SVM)**을 학습
* 커널 기법을 통한 **비선형 분리 문제 해결**

---

## 📏 SVM 개념: 최대 마진 선형 분리

### 🧱 목표

* 양성(positive)과 음성(negative) 샘플을 **가장 넓은 도로(widest street)**로 구분하는 선형 결정경계 도출

### 🔑 핵심 조건

* 결정 경계는 $w \cdot x + b = 0$
* 샘플은 다음을 만족해야 함:

  * $$y_i (w \cdot x_i + b) \geq 1$$
  * $$y_i \in {+1, -1}$$

> 💡 $w$는 결정 경계에 수직인 벡터이며, 마진은 $\frac{2}{|w|}$로 표현됨

---

## 🎯 목적 함수와 라그랑지 승수법

### 🧠 목적

* **마진 최대화 ⇨ $|w|$ 최소화 ⇨ $\frac{1}{2} |w|^2$ 최소화**

### 🎯 라그랑지언(Lagrangian)

$$
L = \frac{1}{2}\|w\|^2 - \sum_i \alpha_i [y_i(w \cdot x_i + b) - 1]
$$

### 📌 파생 조건들

* $$\frac{\partial L}{\partial w} = w - \sum_i \alpha_i y_i x_i = 0 \Rightarrow w = \sum_i \alpha_i y_i x_i$$
* $$\frac{\partial L}{\partial b} = \sum_i \alpha_i y_i = 0$$

---

## 🎼 수학의 합창: 최종 목적 함수

$$
L = \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)
$$

> 💡 이 목적함수는 **샘플 간 내적(dot product)**만을 기반으로 구성됨 → 커널 기법의 기반

### 🎯 최종 결정 함수

$$
\text{sign} \left(\sum_i \alpha_i y_i (x_i \cdot u) + b \right)
$$

---

## 🔁 커널 트릭(Kernel Trick): 비선형 문제 해결

### 💡 문제

* 샘플이 **선형 분리가 불가능(linearly inseparable)**할 경우, SVM은 한계 존재

### 🧪 해결

* 고차원 공간으로 매핑: $\phi(x)$
* 커널 함수 $K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$ 정의
* **직접 매핑 없이도 고차원 내적 결과만 계산 가능**

### ⚙️ 예시 커널

| 이름                        | 수식                                           |
| --------------------------- | ---------------------------------------------- |
| 다항 커널 (Polynomial)      | $K(u, v) = (u \cdot v + 1)^n$                  |
| RBF (Radial Basis Function) | $K(u, v) = e^{-\frac{\|u - v\|^2}{2\sigma^2}}$ |

> 💡 **고차원 매핑 없이도 비선형 분류 가능**, 단, sigma 선택에 따라 **과적합 위험 존재**

---

## ✅ SVM의 장점 vs 신경망

| 항목        | SVM                         | Neural Net                 |
| ----------- | --------------------------- | -------------------------- |
| 최적화 구조 | **Convex** (전역 최적 보장) | 비볼록 (Local minima 위험) |
| 계산 대상   | 벡터 간 내적                | 계층별 비선형 조합         |
| 커널 확장   | 가능                        | 제한적                     |

---

## 📚 역사와 통찰

* Vapnik의 박사논문(1960s, 소련): SVM 아이디어 존재
* **1990s**: 미국 이민 후, Bell Labs에서 손글씨 인식 문제에 SVM 적용
* NIPS에 제출한 3편 논문 모두 **거절**, 이후 커널 기법 발전
* 커널의 위력 발견: **단순 선형 문제에 고차원 특징 부여 가능**
* Vapnik은 이후 **기계학습의 거장**으로 자리잡음

> 💡 교훈: 위대한 아이디어는 종종 **수십년 뒤** 조명받는다

---

## 🧠 정리

* SVM은 **마진을 최대화**하여 일반화 성능을 확보하는 분류 기법
* 수학적으로는 **볼록 최적화(convex optimization)**
* **커널 트릭을 이용해** 비선형 문제까지 일반화
* 최종 분류/결정은 **샘플과의 내적 정보**만을 기반으로 이루어짐

> 💬 핵심 메시지: **문제를 해결 못할 때는 시야를 바꾸라 (change perspective)**
