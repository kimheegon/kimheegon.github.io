---

title: "[MIT6.034] 6.게임과 Minimax, Alpha-Beta, Progressive Deepening"
date: 2025-06-04 17:00:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034]
math: true
---

**MIT AI 강의 6 - 게임 탐색: Minimax, Alpha-Beta, Progressive Deepening**

---

# **게임 그리고 지능**
* MIT 철학자 **Hubert Dreyfus**는 1963년 "컴퓨터는 체스를 둘 수 없다" 주장
* AI 연구자들과 컴퓨터 체스 대결 → **Greenblatt 체스 머신**에게 패배
* **Deep Blue(1997)** 가 세계 챔피언을 이긴 이후 체스는 인간 중심 게임이 아님을 입증
* 오늘날 체스는 **지능 모델링**의 사례로 사용됨


## 컴퓨터 체스 접근 방식

### 1. **휴먼 분석 모사**

* 말 구조, 왕의 안전, 캐슬링 타이밍 등 **전략적 평가와 전술**로 판단
* 현실적으로 구현 불가 → 대부분의 프로그램은 사용하지 않음

### 2. **조건부 규칙 기반 (if-then rules)**

* 예: "퀸 앞폰 움직일 수 있으면 그렇게 해라"
* 평가나 탐색 없이 즉시 행동 선택 → **간단한 게임에는 가능하지만 체스에는 부적절**

### 3. **보드 평가 기반 탐색 (look-ahead + evaluation)**

* 다음 가능한 보드 상태를 미리 보고 가장 좋은 결과를 선택
* 각 상태는 **정적 평가 함수(static evaluation function)** 로 점수화
  > 정적 평가는 보드의 여러 특징 (f1, f2, ...)을 기반으로 선형 다항식으로 구성됨:

$$
V = c_1 f_1 + c_2 f_2 + \dots + c_n f_n
$$

---

## **브리티시 뮤지엄 알고리즘 (British Museum Algorithm)**

* 모든 가능한 수를 전개 → 각 리프 노드에서 점수 평가
* 분기 계수 $b$, 깊이 $d$일 때 리프 노드 수는 $b^d$
* 체스의 경우 평균 $b = 14$, $d = 100$ → $10^{115}$개의 노드 평가 필요 ❌

* 우주 계산 자원과 비교
  > * 우주 원자 수: $10^{80}$
  > * 나노초 단위 우주 수명 계산량: $10^{106}$
  > * 체스 전체 탐색에는 $10^{120}$ 필요 → **불가능**

---

## **현실적인 체스 탐색: Minimax**

### **Minimax 알고리즘**

* **최대화 플레이어 (Maximizer)**: 가능한 한 높은 점수를 선택
* **최소화 플레이어 (Minimizer)**: 가능한 한 낮은 점수를 유도
* 리프 노드의 정적 값을 기준으로 **값을 백업(Back-up)** 하며 의사결정

  > **예시**:

            [?]  ← MAX
          /    \
        [?]     [?] ← MIN  
      /   \    /   \  
      2      7 1    8


  > * 트리 깊이 2, 분기계수 2
  > * 리프 노드: 2, 7, 1, 8
  > * 최종 결정: Minimax 기준으로, Max 플레이어가 Min의 반응을 고려해 점수 2를 선택함
    >> 트리의 루트 노드에서 Max가 시작하며, 자식 노드들이 Min인 경우 각각의 리프 노드 값을 본다. 
    >> 왼쪽 서브트리는 Min이 2 선택, 오른쪽 서브트리는 Min이 1 선택  
    >> → Max는 둘 중 큰 값인 2를 선택


  > **실제 게임 적용 예시**:

      MAX (내 턴)  
      ├── MIN (상대 턴)  
      │ ├── MAX (내 턴)  
      │ │ └── ...  

  >  * **MAX**: 내 턴 → 가장 유리한 수 선택  
  >  * **MIN**: 상대 턴 → 내가 가장 불리한 수를 상정하여 대비
    > 루트 노드에서 최고 점수를 만드는 수가 최종 선택됨

#### **리프 노드 평가 (휴리스틱 함수)**

* 리프 노드에선 게임이 끝나지 않음 → **정적 평가 점수 사용**
* 평가 기준 예시:
  - 기물 점수 합산 (퀸 > 룩 > 나이트 > 폰 등)
  - 킹의 안전성
  - 활동성, 지배력
  - 센터 장악 여부 등
  > 예:  
  > $\text{score(board)} = +3.2$ → 내가 유리  
  > $\text{score(board)} = -1.1$ → 상대가 유리


---


##  **알파-베타 가지치기** (Alpha-Beta Pruning)

* Minimax의 계산량을 줄이기 위한 기법
* **불필요한 노드의 정적 평가를 생략**해 탐색 효율 증가
* **핵심 아이디어**:
  * **Alpha ($\alpha$)**: 최대화 플레이어가 보장받은 최댓값 하한
  * **Beta ($\beta$)**: 최소화 플레이어가 보장받은 최솟값 상한

### **가지치기 조건**:

* 어떤 분기에서 이미 $\alpha \geq \beta$ 가 되면 **해당 하위 트리는 탐색하지 않음**
  > **예시**: 왼쪽 트리에서 최소 2를 보장받은 후, 오른쪽 트리에서 최대 1 → 해당 가지 무시

* **효과**:
  * 계산량 이론적 감소: $2 \cdot b^{d/2}$ vs. 원래는 $b^d$
  > **예시**: 7단계까지만 가능했던 것이 14단계까지 가능해짐


#### **질문**
  - **현실에서는 깊이 제한이 있을 건데 가지치기로 너무 얕은 정보만 보고 pruning 하면 문제가 되지 않을까??**
    * 깊이 제한 없이 전체 탐색 불가 → 결국 리프 노드에서 정적 평가에 의존해야 함
    * 해결 방법: Quiescence Search (퀴센스 탐색)
      - 단순 정적 평가가 불안정할 수 있는 상황을 좀 더 깊이 탐색해서 보완함.  
        > 정적 평가 함수가 혼란스러운(불안정한) 상황을 잘못 평가하지 않도록 그 상황이 "조용해질 때까지" 더 깊이 탐색하는 방법입니다.
        > **예시 상황**
        > - 어떤 리프 노드는 말을 잡고 있는 상황
        > - 다음 턴에 내 말이 다시 잡히거나, 체크메이트로 연결될 수 있음
        > - 그런데 정적 평가 함수는 지금 당장만 보고 점수 계산


---

## **Progressive Deepening (점진적 심화)**

* 탐색 깊이를 고정하지 않고 **시간에 따라 점진적으로 탐색 깊이 확장**
* 언제든 탐색을 중단해도 **의미 있는 답(보험용 값)**을 얻을 수 있음
  > **계산 예시**:
  >  * 리프 노드 수: $b^d$
  >  * 그 전 단계 (d-1)의 수: $b^{d-1}$ → 비용은 약 10% 수준
  >  * **모든 단계에서 보험값 계산하면 전체 비용은 여전히 $O(b^d)$ 수준**

* 계산 용도:
  * 제한 시간 내 응답이 필요한 상황에서 사용
    > **예시**: Deep Blue는 약 2분 내 한 수를 둬야 하므로 progressive deepening이 필요
    > - 체스 AI는 시간 제한 안에서 생각해야 함 (예: 한 수 3초 안에 두기)
    > - 그냥 깊이 d까지 한 번에 탐색하려고 하다 시간 초과나면 → 아무 수나 둬야 할 수도 있음 (👎)
    > - 그래서, 먼저 깊이 1까지 탐색 → 최선 수 저장
    > - 시간 여유 있으면 깊이 2까지 재탐색
    > - 또 여유 있으면 깊이 3까지…
    > - 이렇게 점점 깊게 재탐색하는 방식
    > - 탐색 도중 언제든 멈춰도 **직전 단계에서 얻은 “보험값”**으로 둬도 됨 (괜찮은 수)
    >  - b = 10이라고 가정하면, 깊이에 따라서 계산 비중이 10배씩 늘어나므로 다시 계산한다고 손해를 보지않음.

---

## **Deep Blue의 구성**

* Minimax + Alpha-Beta + Progressive Deepening
* 초당 2억 개 정적 평가
* 약 14~16 단계까지 탐색
* Opening book, Endgame table, **Uneven tree expansion** 포함

  > 🚀 Uneven tree: 특정 동적 상황(예: 퀸 캡처 가능)에서 더 깊은 탐색 허용

---

## **인간과 AI의 사고 차이**

* 인간: **패턴 인식과 전략 지식** 기반 판단
* Deep Blue: **엄청난 계산력** 기반 → Bulldozer-style Intelligence
  > 인간은 무작위 배치된 체스판은 기억 못하지만, 실제 게임상황은 잘 기억함 → **패턴 기반** 학습 결과

---

## ✅ **정리**

* **Minimax**: 게임 트리에서 최선의 수를 역산하는 기본 알고리즘
* **Alpha-Beta**: 계산량을 극적으로 줄이는 가지치기 기법
* **Progressive Deepening**: **시간 제한에 대응하고 항상 유효한 수를 보장하는 탐색 구조**
* **Deep Blue**: 이 구조들 + 병렬 연산 + 사전 지식 + uneven search로 세계 챔피언 격파
  > 인간처럼 생각하지는 않지만, **지능의 한 축**으로서의 계산형 지능을 모델링함
