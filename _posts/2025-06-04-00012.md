---

title: "[MIT6.034] 12.뉴럴 네트워크(Neural Networks) 기초와 학습 원리"
date: 2025-06-04 23:10:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, Neural-Network, Backpropagation, Learning]
math: true
---

**🧠 MIT AI 강의 12 - 뉴럴 네트워크(Neural Network)의 작동 원리와 학습 알고리즘**

---

## 🧭 도입: 신경망, 버릴 뻔했던 주제

* 2010년까지 MIT 6.034 커리큘럼에서 **신경망 제거 고려**됨
* 이유: 실제 뇌와 유사하지 않음, 성능 미비
* 하지만 **2012년 Jeff Hinton의 이미지 분류 성공**으로 관심 폭증

  * 1,000 클래스 중 최고 확률 5개 예측 (top-5 accuracy)
  * 정확한 예측도 많지만 오류 사례도 존재 (ex. 개와 고양이 오인)

---

## 🧬 생물학적 뉴런 구조

* 구성 요소:

  * **세포체(cell body)**, **핵(nucleus)**
  * **축삭(axon)**: 출력, **수상돌기(dendrite)**: 입력 수신
  * **시냅스(synapse)**: 뉴런 간 정보 전달 지점

### ⚡ 신호 전달 메커니즘

* 충분한 자극 → **스파이크(spike)** 발생 → 축삭 통해 신호 전송
* 이후 **불응기(refractory period)** 진입
* 시냅스에서 **신경전달물질** 방출되어 다음 뉴런 자극

---

## 🔢 신경망 모델의 수학적 구조

* 입력 $$x_i$$ (0 또는 1): **이진 입력**
* 가중치 $$w_i$$와 곱하여 누적합 계산
* 누적값이 임계값 $$t$$ 넘으면 출력 $$z = 1$$, 아니면 $$z = 0$$

### 📦 구성 정리

1. **가중치(weight)**: 시냅스 세기 반영
2. **합산기(summer)**: 누적 자극 계산
3. **임계함수(threshold function)**: 출력 결정

> 단점: 불연속 함수(step function) → 미분 불가

---

## 🎯 신경망은 무엇을 하는가?

> **신경망 = 함수 근사기(Function Approximator)**

* 목표: 입력 벡터 $$x$$ → 출력 벡터 $$z$$
* 훈련 시: 가중치 $$w$$, 임계값 $$t$$ 조정하여 $$z$$를 원하는 출력으로 맞춤

---

## 📉 성능 측정과 최적화 목표

* 성능 함수 $$P$$ 정의:

$$
P = -\frac{1}{2}(d - z)^2
$$

* $$d$$: 원하는 출력, $$z$$: 실제 출력
* **P 최대화** 또는 오차 최소화 목적

### ⛰️ 최적화 전략

* 단순한 경우: **Hill Climbing** 가능
* 고차원(예: 60M 파라미터)에서는 비현실적
* 해법: **경사 하강법(Gradient Descent)**

$$
\Delta w = -R \cdot \nabla P
$$

* $$R$$: 학습률(rate constant)
* 방향: 성능 증가 방향 (기울기 따라 이동)

---

## ⚠️ 문제점 1: 불연속 함수 → 미분 불가

* **Step function**은 미분 불가능
* 해결책: **시그모이드 함수(sigmoid)** 사용

$$
\sigma(\alpha) = \frac{1}{1 + e^{-\alpha}}
$$

* 출력에 대한 미분도 간단:

$$
\frac{d\sigma}{d\alpha} = \sigma(\alpha)(1 - \sigma(\alpha))
$$

## ⚠️ 문제점 2: 임계값 처리

* Trick: 입력에 **항상 -1인 가상 입력 추가**, 가중치 $$w_0 = t$$ 설정
* 결과적으로 threshold 처리를 **가중치 하나로 통합** 가능

---

## 🧪 역전파 학습 (Backpropagation)

### 🎯 목표: 성능함수 $$P$$를 **각 가중치 $$w$$에 대해 미분**

* 예: $$\frac{\partial P}{\partial w_1}$$
* 체인룰 사용해 연쇄적으로 미분 계산:

$$
\frac{\partial P}{\partial w_1} = \frac{\partial P}{\partial z} \cdot \frac{\partial z}{\partial p_2} \cdot \frac{\partial p_2}{\partial y} \cdot \frac{\partial y}{\partial p_1} \cdot \frac{\partial p_1}{\partial w_1}
$$

* 각 중간 단계는 시그모이드 함수의 특성과 선형 곱으로 구성되어 쉽게 계산 가능

---

## 🧪 실습 시연 및 학습률 튜닝

* 실습: 입력과 출력이 같은 가장 단순한 네트워크 학습
* 초기 가중치는 **무작위(random initialization)** 필요
* 학습률 $$R$$ 조정 필요:

  * 너무 작으면 수렴 느림
  * 너무 크면 진동 발생 및 발산 가능

---

## 🧱 깊은 신경망과 계산량

* 깊이(depth)가 늘어나면 계산량 **선형 증가**
* 너비(width)는 노드 간 연결 수에 따라 **제곱 비례**
* **재사용 원리(reuse principle)**:

  * 역전파 시, **이전 층에서 계산된 결과를 재활용**하여 계산량 최소화
  * FFT(Fast Fourier Transform)와 유사 개념

---

## ✅ 정리

* 신경망은 **입력과 출력 간 함수 근사 구조**
* 학습 목표는 **출력과 원하는 값 사이 오차 최소화**
* 효율적인 학습 위해:

  1. 시그모이드로 연속화
  2. threshold를 가중치로 통합
  3. 경사하강법과 체인룰 기반 역전파 사용
* 깊은 신경망에서도 계산량 폭증 없이 **선형 시간 처리 가능**
* 핵심 개념: **간단한 구조 + 재사용 아이디어 + 수학적 최적화 기법**
