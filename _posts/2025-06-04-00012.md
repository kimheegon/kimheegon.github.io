---

title: "[MIT6.034] 12.뉴럴 네트워크(Neural Networks) 기초와 학습 원리"
date: 2025-06-09 23:10:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, Neural-Network, Backpropagation, Learning]
math: true
---

**🧠 MIT AI 강의 12 - 뉴럴 네트워크(Neural Network)의 작동 원리와 학습 알고리즘**

---

## 🧭 도입: 신경망, 버릴 뻔했던 주제

* 2010년까지 MIT 6.034 커리큘럼에서 **신경망 제거 고려**됨
* 이유: 실제 뇌와 유사하지 않음, 성능 미비
* 하지만 **2012년 Jeff Hinton의 이미지 분류 성공**으로 관심 폭증

  * 1,000 클래스 중 최고 확률 5개 예측 (top-5 accuracy)
  * 정확한 예측도 많지만 오류 사례도 존재 (ex. 개와 고양이 오인)

---

## 🧬 생물학적 뉴런 구조

* 구성 요소:

  * **세포체(cell body)**, **핵(nucleus)**
  * **축삭(axon)**: 출력, **수상돌기(dendrite)**: 입력 수신
  * **시냅스(synapse)**: 뉴런 간 정보 전달 지점

### ⚡ 신호 전달 메커니즘

* 충분한 자극 → **스파이크(spike)** 발생 → 축삭 통해 신호 전송
* 이후 **불응기(refractory period)** 진입
* 시냅스에서 **신경전달물질** 방출되어 다음 뉴런 자극

---

## 🔢 신경망 모델의 수학적 구조

* 입력 $$x_i$$ (0 또는 1): **이진 입력**
* 가중치 $$w_i$$와 곱하여 누적합 계산
* 누적값이 임계값 $$t$$ 넘으면 출력 $$z = 1$$, 아니면 $$z = 0$$

### 📦 구성 정리

1. **가중치(weight)**: 시냅스 세기 반영
2. **합산기(summer)**: 누적 자극 계산
3. **임계함수(threshold function)**: 출력 결정

> 단점: 불연속 함수(step function) → 미분 불가

---

## 🎯 신경망은 무엇을 하는가?

> **신경망 = 함수 근사기(Function Approximator)**

* 목표: 입력 벡터 $$x$$ → 출력 벡터 $$z$$
* 훈련 시: 가중치 $$w$$, 임계값 $$t$$ 조정하여 $$z$$를 원하는 출력으로 맞춤

---

## 📉 성능 측정과 최적화 목표

* 성능 함수 $$P$$ 정의:

$$
P = -\frac{1}{2}(d - z)^2
$$

* $$d$$: 원하는 출력, $$z$$: 실제 출력
* **P 최대화** 또는 오차 최소화 목적

### ⛰️ 최적화 전략

* 단순한 경우: **Hill Climbing** 가능
* 고차원(예: 60M 파라미터)에서는 비현실적
* 해법: **경사 하강법(Gradient Descent)**

$$
\Delta w = -R \cdot \nabla P
$$

* $$R$$: 학습률(rate constant)
* 방향: 성능 증가 방향 (기울기 따라 이동)

---

## ⚠️ 문제점 1: 불연속 함수 → 미분 불가


$$
f(x) = 
\begin{cases}
1 & \text{if } x > 0 \\
0 & \text{otherwise}
\end{cases}
$$

* 불연속이므로 $$x = 0$$에서 미분 불가
* $$x < 0$$과 $$x > 0$$에서도 도함수가 0 → **학습 불가**
* 기울기 하강법(gradient descent)에 사용 불가능
* 해결책: **시그모이드 함수(sigmoid)** 사용

$$
\sigma(\alpha) = \frac{1}{1 + e^{-\alpha}}
$$

| 항목 | 설명 |
|------|------|
| 출력 범위 | (0, 1) |
| 미분 가능성 | 전체 실수 구간에서 가능 |
| 해석 | 확률로 해석 가능 |


* 출력에 대한 미분도 간단:

$$
\frac{d\sigma}{d\alpha} = \sigma(\alpha)(1 - \sigma(\alpha))
$$


### ❌ 시그모이드의 한계

| 문제 | 설명 |
|------|------|
| Gradient Vanishing | $$x$$가 너무 크거나 작으면 미분값이 0에 가까움 → 역전파 멈춤 |
| Saturation | 출력이 0 또는 1로 수렴하면 가중치 업데이트가 거의 없음 |
| 출력 중심이 0 아님 | 평균 0이 아니어서 학습이 느려짐 |

---


### 🚀 대안: ReLU 함수 (Rectified Linear Unit)

$$
\text{ReLU}(x) = \max(0, x)
$$

#### 특징

| 항목 | 설명 |
|------|------|
| 출력 범위 | $$[0, \infty)$$ |
| 단순한 구조 | $$(x > 0)$$이면 선형, $$(x < 0)$$이면 0 |
| 계산 효율 | 매우 빠름, 깊은 네트워크에서도 안정적 |
| 학습 가능성 | 대부분의 영역에서 미분 가능 → 경사하강법 사용 가능 |

---

### 📌 ReLU의 미분

$$
\frac{d}{dx} \text{ReLU}(x) = 
\begin{cases}
0 & \text{if } x < 0 \\
? & \text{if } x = 0 \\
1 & \text{if } x > 0
\end{cases}
$$

* $$(x = 0)$$에서는 미분 불가능  
* 실전에서는 0 또는 1을 **임의로 정의**하여 사용 → 문제 없음

---

### ❓ "임의로 정의해도 된다면, Step 함수도 쓰지 그랬나?"

* ❌ Step 함수는 **모든 곳에서 기울기 = 0**, 학습 불가능
* ✅ ReLU는 **거의 모든 곳에서 미분 가능**, 단 하나의 점만 특이
* 따라서 기울기 하강법이 가능한 구조

---

### ⚠️ ReLU의 실제 문제: 죽은 뉴런 (Dead Neuron)

* $$(x < 0)$$에서는 출력이 0 → 기울기 0 → 가중치가 계속 업데이트 안 됨
* 뉴런이 **영구적으로 비활성화**되는 현상 발생

---

### 🔧 해결 방법: ReLU 변형 함수들

| 함수 | 수식 | 설명 |
|------|------|------|
| **Leaky ReLU** | $$( \max(0.01x, x) )$$ | 음수 구간에도 작은 기울기 유지 |
| **Parametric ReLU** | $$( \max(ax, x) )$$ | $$a)$$를 학습으로 결정 |
| **ELU** | $$( x )$$ 또는 $$( \alpha(e^x - 1) )$$ | 부드러운 경계 |
| **GELU** | 확률적 연속 함수 | 최근 트랜스포머 기반 모델에 널리 사용됨 |

---



## ⚠️ 문제점 2: 임계값 처리 (Threshold Handling)

---

### 🧠 문제 배경: 퍼셉트론의 구조

퍼셉트론의 출력 정의:

$$
\text{출력} =
\begin{cases}
1 & \text{if } \sum_i w_i x_i \geq t \\
0 & \text{otherwise}
\end{cases}
$$

| 기호 | 의미 |
|------|------|
| $$( w_i )$$ | 각 입력의 가중치 |
| $$( x_i )$$ | 입력 값 |
| $$( t )$$ | **임계값 (threshold)** — 이 값 이상이면 1 출력 |

---

### ❗ 문제점: threshold는 수식에서 **따로 떨어져 있음**

* 퍼셉트론 수식에서 threshold는 다른 항으로 분리됨
* 학습이나 미분 적용 시 **수식 일관성이 떨어짐**
* 구조상 번거로움

---

## ✅ 해결책: Trick — 가상 입력 추가

### 💡 아이디어

* 항상 입력 $$( x_0 = -1 )$$을 추가하고
* 대응 가중치 $$( w_0 = t )$$로 설정하면

원래 수식:

$$
\sum_i w_i x_i \geq t
$$

를 이렇게 표현 가능:

$$
w_0 x_0 + \sum_{i=1}^n w_i x_i = -t + \sum w_i x_i \geq 0
$$

즉:

$$
\sum_{i=0}^n w_i x_i \geq 0
$$

📌 threshold를 **가중치 항 내에 통합**!

---

## 🎯 장점

| 장점 | 설명 |
|------|------|
| ✅ 수식 간결화 | threshold가 제거되어 **내적 표현만 남음** |
| ✅ 계산 용이 | 경사하강법 등 미분 수식 적용이 쉬움 |
| ✅ 구조 통일 | 모든 뉴런의 출력이 동일한 수식으로 표현됨 |
| ✅ 일반화 가능 | 이후 다층 퍼셉트론, 신경망에도 그대로 적용 가능

---

## 🧱 일반화: Bias Term

| 용어        | 의미 |
|-------------|------|
| Threshold $$( t )$$ | 신호 발화 기준 값 |
| Bias $$( b = -t )$$ | threshold를 내부화한 값 |
| 표현 방식 | $$( \vec{w} \cdot \vec{x} + b \geq 0 )$$ |

→ ✅ 현대 신경망에서는 **bias 항**으로 표현  
→ **threshold는 개념적으로 bias로 대체됨**

---


## 🧪 역전파 학습 (Backpropagation)

### 🎯 목표: 성능함수 $$P$$를 **각 가중치 $$w$$에 대해 미분**

* 예: $$\frac{\partial P}{\partial w_1}$$
* 체인룰 사용해 연쇄적으로 미분 계산:

$$
\frac{\partial P}{\partial w_1} = \frac{\partial P}{\partial z} \cdot \frac{\partial z}{\partial p_2} \cdot \frac{\partial p_2}{\partial y} \cdot \frac{\partial y}{\partial p_1} \cdot \frac{\partial p_1}{\partial w_1}
$$

* 각 중간 단계는 시그모이드 함수의 특성과 선형 곱으로 구성되어 쉽게 계산 가능

---

## 🧪 실습 시연 및 학습률 튜닝

* 실습: 입력과 출력이 같은 가장 단순한 네트워크 학습
* 초기 가중치는 **무작위(random initialization)** 필요
* 학습률 $$R$$ 조정 필요:

  * 너무 작으면 수렴 느림
  * 너무 크면 진동 발생 및 발산 가능

---

## 🧱 깊은 신경망과 계산량

* 깊이(depth)가 늘어나면 계산량 **선형 증가**
* 너비(width)는 노드 간 연결 수에 따라 **제곱 비례**
* **재사용 원리(reuse principle)**:

  * 역전파 시, **이전 층에서 계산된 결과를 재활용**하여 계산량 최소화
  * FFT(Fast Fourier Transform)와 유사 개념

---

## ✅ 정리

* 신경망은 **입력과 출력 간 함수 근사 구조**
* 학습 목표는 **출력과 원하는 값 사이 오차 최소화**
* 효율적인 학습 위해:

  1. 시그모이드로 연속화
  2. threshold를 가중치로 통합
  3. 경사하강법과 체인룰 기반 역전파 사용
* 깊은 신경망에서도 계산량 폭증 없이 **선형 시간 처리 가능**
* 핵심 개념: **간단한 구조 + 재사용 아이디어 + 수학적 최적화 기법**

---

## ✅ 예제



$$
x \rightarrow \text{Linear1} \rightarrow a_1 \rightarrow \text{ReLU} \rightarrow h_1 \rightarrow \text{Linear2} \rightarrow a_2 \rightarrow \text{Sigmoid} \rightarrow \hat{y} \rightarrow \text{Loss } L
$$

- 입력: $$x \in \mathbb{R}^n$$
- 출력: $$\hat{y} \in \mathbb{R}$$
- 정답: $$y \in \mathbb{R}$$
- 손실 함수: MSE (Mean Squared Error)

---

### 🔄 순전파 (Forward Pass)

#### ① Linear1

$$
a_1 = W_1 x + b_1
$$

- 저장: $$x$$, $$a_1$$
- 이유: $$x$$는 $$dW_1$$ 계산에, $$a_1$$은 ReLU 미분에 필요

---

#### ② ReLU

$$
h_1 = \text{ReLU}(a_1) =
\begin{cases}
a_1 & \text{if } a_1 > 0 \\
0 & \text{otherwise}
\end{cases}
$$

- 저장: $$h_1$$
- 이유: Linear2 입력값 및 역전파 대상

---

#### ③ Linear2

$$
a_2 = W_2 h_1 + b_2
$$

- 저장: $$a_2$$
- 이유: Sigmoid 입력 → 도함수 계산 필요

---

#### ④ Sigmoid

$$
\hat{y} = \sigma(a_2) = \frac{1}{1 + e^{-a_2}}
$$

- 저장: $$\hat{y}$$
- 이유: 손실 함수 및 sigmoid 도함수 계산

---

#### ⑤ Loss (MSE)

$$
L = \frac{1}{2} (\hat{y} - y)^2
$$

- 출력값: $$L$$

---

### 🔁 역전파 (Backward Pass) 및 미분 유도

---

#### Step 1: Loss 함수 도함수

손실 함수가:

$$
L = \frac{1}{2} (\hat{y} - y)^2
$$

미분하면:

$$
\frac{\partial L}{\partial \hat{y}} = \hat{y} - y
$$


* 왜 $$\hat{y}$$ 에대해 미분하는가?
* $$L$$은 $$\hat{y}$$에 의해 정의되기 때문
> 신경망은 오차 $$L$$를 줄여야 하고, 이를 위해선 $$\hat{y}$$가 $$L$$에 얼마나 영향을 주는지 알아야 함

엄밀히 말한다면 아래와 같음 우리가 직접적으로 원하는것은?
> 손실 $$L$$을 줄이기 위해 가중치 $$W$$를 업데이트하는 것

그런데 $$L$$은 $$W$$에 직접적으로 연결되어 있지 않음
→ $$L$$은 $$\hat{y}$$에 의존하고,
→ $$\hat{y}$$는 $$a$$에 의존하고,
→ $$a$$는 $$W$$에 의존함

따라서 Chain Rule을 만들고 각각에 대해서 미분을 실시하는 것임.



---

#### Step 2: Sigmoid 도함수 유도

Sigmoid 함수 정의:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

미분하면:

$$
\frac{d\sigma}{dz} = \frac{d}{dz} \left( \frac{1}{1 + e^{-z}} \right)
= \frac{e^{-z}}{(1 + e^{-z})^2}
= \sigma(z)(1 - \sigma(z))
$$

즉:

$$
\frac{\partial \hat{y}}{\partial a_2} = \hat{y}(1 - \hat{y})
$$

연쇄법칙(chain rule) 적용:

$$
\frac{\partial L}{\partial a_2} =
\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_2}
= (\hat{y} - y) \cdot \hat{y}(1 - \hat{y})
$$

---

#### Step 3: Linear2 도함수 유도

선형함수: $$a_2 = W_2 h_1 + b_2$$

도함수:

- 가중치에 대해:

$$
\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial a_2} \cdot h_1^T
$$

- 편향에 대해:

$$
\frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial a_2}
$$

- 입력 $$h_1$$에 대한 미분:

$$
\frac{\partial L}{\partial h_1} = W_2^T \cdot \frac{\partial L}{\partial a_2}
$$

---

#### Step 4: ReLU 도함수

$$
\frac{d}{da_1} \text{ReLU}(a_1) =
\begin{cases}
1 & a_1 > 0 \\
0 & a_1 \leq 0
\end{cases}
$$

따라서:

$$
\frac{\partial L}{\partial a_1} = \frac{\partial L}{\partial h_1} \cdot \text{ReLU}'(a_1)
$$

---

#### Step 5: Linear1 도함수 유도

선형함수: $$a_1 = W_1 x + b_1$$

- 가중치 미분:

$$
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial a_1} \cdot x^T
$$

- 편향 미분:

$$
\frac{\partial L}{\partial b_1} = \frac{\partial L}{\partial a_1}
$$

---

### ✅ 전체 계산 순서 요약

1. $$\frac{\partial L}{\partial \hat{y}} = \hat{y} - y$$
2. $$\frac{\partial L}{\partial a_2} = (\hat{y} - y) \cdot \hat{y}(1 - \hat{y})$$
3. $$\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial a_2} \cdot h_1^T$$
4. $$\frac{\partial L}{\partial h_1} = W_2^T \cdot \frac{\partial L}{\partial a_2}$$
5. $$\frac{\partial L}{\partial a_1} = \frac{\partial L}{\partial h_1} \cdot \text{ReLU}'(a_1)$$
6. $$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial a_1} \cdot x^T$$


우리가 원하는것은 $$\frac{\partial L}{\partial W_2}$$ 과 $$\frac{\partial L}{\partial W_1}$$ 이며 순차적으로 미분하면서 최종적으로 $$W_1$$ 까지 미분값을 구할 수 있게 됨.
> 재미있는 것은 각 미분값들은 local gradient 값인 $$\frac{\partial L}{\partial a_2}$$ 와 $$\frac{\partial L}{\partial a_1}$$ 로 표현됨.

---

### 🔚 결론

- 역전파는 순전파에서 나온 값들을 바탕으로 체인 룰을 통해 오차를 전파
- $$\hat{y}$$, $$a_2$$, $$h_1$$, $$a_1$$, $$x$$는 역전파에서 반드시 필요
- 딥러닝 프레임워크는 이 값들을 자동으로 저장하여 역전파에서 재사용함
