---

title: "[MIT6.034] 11.식별 트리와 엔트로피 기반 학습"
date: 2025-06-04 22:00:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, Learning, Entropy, Decision-Tree]
math: true
---

**MIT AI 강의 11 - 식별 트리(Identification Trees)와 정보 이론 기반 학습**

---

# **식별 트리와 정보 이론**

- 목표 예제: 뱀파이어 분류
  * 문제 설정: 특성 정보(그림자, 마늘 섭취, 안색, 억양 등) 기반으로 **뱀파이어 여부 판별**
    * **특성은 수치형이 아닌 심볼릭(symbolic)** 으로 정의
  * 목적: 효율적인 **식별 트리(Identification Tree)** 설계


## **식별 트리 구조**

* 테스트의 결과를 따라 분기
* 각 리프 노드는 **동일 클래스(예: 모두 뱀파이어)** 가 되는 것이 이상적
  - 동일한 클래스가 아니면 혼란될 수 있음
    > **예시**: 
    > - 뱀파이어/뱀피/뱀파이어일지도? (BAD)
    > - 뱀파이어/휴먼 (Good)

- 좋은 트리란?
  * **작고 간단한할 수록 좋다** (Occam's Razor)
    * 전체 테스트 수/비용 최소화
  * **균일(homogeneous)** 한 분류 결과를 생성한다

--

## **특성 선택 기준: 균일도 평가**

- **"동질(homogeneous)"**이란?
  * 테스트의 결과로 나뉜 집단(분기)이 **모두 같은 클래스(예: 모두 뱀파이어 또는 모두 인간)** 만 포함하는 경우
- 예시: 4명의 인물에 대해 가능한 첫 번째 테스트 평가

  | 테스트     | 분기 결과                                                 | 동질 집합 수 |
  | ---------- | --------------------------------------------------------- | ------------ |
  | **그림자** | 없음: 3명 → 모두 뱀파이어  ✅<br>있음: 1명 → 인간 ✅        | ✅ 2          |
  | 마늘       | 있음: 2명 (1 뱀파, 1 인간)<br>없음: 2명 (1 뱀파, 1 인간)  | ❌ 0          |
  | 안색       | 창백: 2명 → 모두 뱀파이어 ✅<br>정상: 2명 (1 뱀파, 1 인간) | ✅ 1          |
  | 억양       | 각 분기에서 인간/뱀파이어가 섞임                          | ❌ 0          |

- 최적 선택 (from 예시)
  - 최적 특성: 그림자 테스트
    * **그림자가 없으면 → 전원 뱀파이어**
    * **그림자가 있으면 → 인간**
    * 두 분기가 모두 **동질적**이므로, **첫 테스트로 가장 이상적**

---

## **대규모 데이터 처리: 엔트로피 기반 평가**

### **엔트로피 공식 (불균일도)**

$$
D = - \frac{P}{T} \log_2 \left(\frac{P}{T}\right) - \frac{N}{T} \log_2 \left(\frac{N}{T}\right)
$$

* P: 양성 샘플 수, N: 음성 샘플 수, T: 전체 샘플 수
* **균일할수록 엔트로피 낮아지고, 혼합될수록 엔트로피는 증가한다** (최대값: 1)

---

## **정보량과 엔트로피**

### **정보량**
- 정보량이라는 개념은 왜 필요한가?
  * 우리가 어떤 사건을 관찰했을 때, **그 사건이 얼마나 놀랍거나 예측 불가능했는지**를 수치로 표현하고 싶음
    > **예시**: 주사위는 어떤 숫자가 나올지 몰라서 "정보가 많다"  → 반면 주사위가 항상 3이 나오면 정보가 없다


### **Claude Shannon의 조건**

- Shannon은 “정보량”이라는 개념이 수학적으로 다음 조건들을 만족해야 한다고 보았다:
  1. **확률이 낮을수록 정보량은 커야 한다**  
     → 예: 1% 확률로 벌어진 사건은 더 놀랍다
  2. **독립적인 사건은 정보량이 더해져야 한다**  
     → 동전 + 주사위 → 총 정보량은 둘의 합
  3. **연속적인 변화**  
     → 확률이 조금 바뀌면 정보량도 조금 바뀌어야 한다

#### **샤논의 조건을 만족하는 유일한 수식**:

$$
I(p) = -\log_2(p)
$$

| 기호       | 의미                                     |
| ---------- | ---------------------------------------- |
| $$ p $$    | 어떤 사건이 일어날 확률                  |
| $$ I(p) $$ | 그 사건이 주는 정보량 (bits 단위)        |
| 로그 밑    | 2를 사용하면 정보량 단위는 **비트(bit)** |


### **정보 엔트로피**
- 어떤 확률 분포 전체의 평균 정보량을 뜻함
  $$
  H = -\sum_{i=1}^{n} p_i \log_2 p_i
  $$
  * $$p_i$$: 각 클래스의 확률
  * $$H$$: 전체 시스템의 정보 엔트로피 (불확실성의 총합)



### **이진 분류(binary classification) 적용**
- 클래스가 두 개: 양성(Positive), 음성(Negative)

  $$
  H = -p \log_2 p - (1 - p) \log_2 (1 - p)
  $$

- 샘플 개수로 표현시:

  $$
  H = -\frac{P}{T} \log_2 \left( \frac{P}{T} \right) - \frac{N}{T} \log_2 \left( \frac{N}{T} \right)
  $$

  | 기호          | 의미                             |
  | ------------- | -------------------------------- |
  | $$P$$         | 양성 샘플 수                     |
  | $$N$$         | 음성 샘플 수                     |
  | $$T = P + N$$ | 전체 샘플 수                     |
  | $$H$$         | 현재 집합의 엔트로피 (혼합 정도) |

### **엔트로피의 직관**
- 분포가 **균등할수록 불확실성이 크고**,  
- 한쪽으로 몰릴수록 **정보가 줄어든다**

  | 분포 상태          | 예시      | 엔트로피 |
  | ------------------ | --------- | -------- |
  | 완전한 균형        | 50% / 50% | 1 (최대) |
  | 약간의 불균형      | 80% / 20% | 약 0.72  |
  | 완전한 확정 (편향) | 100% / 0% | 0 (최소) |


## **왜 결정 트리에서 엔트로피를 쓰는가?**

* 어떤 속성(feature)으로 데이터를 나눴을 때, **결과 집합의 엔트로피가 가장 많이 줄어드는** 속성이 **가장 좋은 분할 기준**으로 선택됨
* 이 차이를 정보 이득(information gain)이라고 부른다:

  $$
  \text{Information Gain} = H(\text{전체}) - \sum \frac{T_i}{T} H(T_i)
  $$


  | 항목               | 설명                                                             |
  | ------------------ | ---------------------------------------------------------------- |
  | **정보량**         | 사건이 얼마나 놀랍고 예측 불가능한가                             |
  | **엔트로피**       | 전체 평균 정보량 → 불확실성 측정                                 |
  | **이진 분류 수식** | $$ -\frac{P}{T}\log \frac{P}{T} - \frac{N}{T}\log \frac{N}{T} $$ |
  | **의미**           | 혼합된 정도가 클수록 엔트로피 ↑, 단일 클래스일수록 ↓             |
  | **결정 트리에서**  | 엔트로피를 줄이는 질문(속성)을 골라 분할한다                     |


### **테스트 품질 계산**

* 각 분기별 엔트로피를 계산해 **가중 평균**

  $$
  \text{Test Quality} = \sum_{i} \left(\frac{T_i}{T} \cdot D_i\right)
  $$

  * $$T_i$$: i번째 분기로 내려간 샘플 수
  * $$D_i$$: 해당 분기의 엔트로피
  * **값이 작을수록 좋은 테스트**

---

## **결정 트리에서 Test Quality 계산 예시**


### **시나리오: 뱀파이어 분류**

- 총 6명의 인물에 대해, **뱀파이어인지 인간인지 분류**하려고 함:

  | 이름 | 클래스   |
  | ---- | -------- |
  | A    | 뱀파이어 |
  | B    | 뱀파이어 |
  | C    | 뱀파이어 |
  | D    | 인간     |
  | E    | 인간     |
  | F    | 인간     |

- 테스트 1: “그림자가 있는가?”
  - 결과 분기
    - **분기 1 (그림자 없음)** → A, B, C (전원 뱀파이어)
    - **분기 2 (그림자 있음)** → D, E, F (전원 인간)
  - 각 분기의 엔트로피
    - 분기 1:
      * $$P = 3, N = 0, T = 3$$
      * $$D_1 = -1 \cdot \log_2 1 = 0$$
    * 분기 2:
      * $$P = 0, N = 3, T = 3 $$
      * $$D_2 = -1 \cdot \log_2 1 = 0$$
  - Test Quality:  
      - $$
      \frac{3}{6} \cdot 0 + \frac{3}{6} \cdot 0 = 0
      $$
  - **결론**: 완전 분리된 테스트 → **Test Quality = 0** (이상적)

- 테스트 2: “피부가 창백한가?”
  - 결과 분기
    - **분기 1 (창백함)** → A, B, D (2 뱀파이어 + 1 인간)
    - **분기 2 (창백하지 않음)** → C, E, F (1 뱀파이어 + 2 인간)
  - 각 분기의 엔트로피
    - 분기 1:  
        - $$
        D_1 = -\frac{2}{3}\log_2\frac{2}{3} - \frac{1}{3}\log_2\frac{1}{3}
        ≈ 0.918
        $$
    * 분기 2:
      * $$D_2 ≈ 0.918 $$
  - Test Quality:  
        - $$
        \frac{3}{6} \cdot 0.918 + \frac{3}{6} \cdot 0.918 = 0.918
        $$
  - **비교 결과**:

    | 테스트          | Test Quality |
    | --------------- | ------------ |
    | 그림자 여부     | **0.000** ✅  |
    | 피부색 (창백함) | 0.918 ❌      |
  - 결론:
    - Test Quality는 **테스트 결과로 나뉜 분기들의 엔트로피의 가중 평균**  
    - 값이 **낮을수록 더 잘 분리된 테스트**  
    - 따라서 결정 트리에서 첫 질문으로 **“그림자 여부”**를 선택하는 것이 바람직하다.

### **트리 구축 반복**

| 단계                         | 설명                                                         |
| ---------------------------- | ------------------------------------------------------------ |
| 최상위 테스트 선택           | 현재 전체 샘플을 가장 잘 나누는 속성 선택 (정보이득 최대)    |
| 분기 내 서브셋에 대해 반복   | 각 분기에 대해 **다시 가장 좋은 속성**을 선택해 나눔         |
| 멈추는 조건 (리프 노드 생성) | 샘플이 모두 같은 클래스거나, 더 이상 테스트할 속성이 없을 때 |


### **수치형 특성 처리 방법**

* **특정 임계값(threshold)** 기준으로 양분화
* 예: 체온이 특정 온도보다 높은가?
* 가능한 임계값 후보: **인접 샘플 간 중간값**들
* 반복적 threshold 탐색으로 **엔트로피 최소화 경계 선택**
  > ⚠️ 동일 특성도 threshold 달리하여 **여러 번 재사용 가능**


### **트리 → 규칙 변환**

* 각 리프까지의 경로는 **하나의 규칙**으로 해석 가능
* 예:

  ```text
  IF shadow = ? AND garlic = Yes THEN label = Not Vampire
  ```

* 트리 간소화 가능: **불필요한 조건 제거(규칙 축약)** 가능성 있음


### **트리 방식 vs 최근접 이웃(NN)**

| 구분        | 식별 트리     | 최근접 이웃           |
| ----------- | ------------- | --------------------- |
| 데이터 유형 | 심볼릭 / 수치 | 수치형 주로           |
| 경계 형태   | 축 평행 분할  | 다변량/복잡한 경계    |
| 사용 테스트 | 선택된 일부   | 전체 특성 사용        |
| 실행 비용   | 낮음          | 높음 (거리 계산 다수) |

---

## ✅ 정리

* 식별 트리는 **비수치형, 비용 고려** 분류 문제에 효과적
* 엔트로피 기반 평가는 **복잡도 줄이면서 분류 성능 유지**
* 수치형 데이터도 **threshold 기반** 처리 가능
* 규칙 기반 해석이 가능하여 **설명 가능성(explainability)** 우수
* 실제 분야(의료 진단, 정보 검색 등)에서 **광범위하게 활용됨**
