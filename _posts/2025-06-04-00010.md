---

title: "[MIT6.034] 10.학습: 최근접 이웃, 팔 제어"
date: 2025-06-04 21:00:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, Learning]
math: true
---

**MIT AI 강의 10 - 학습: 최근접 이웃(NN), 모터 제어, 그리고 수면이 뇌에 미치는 영향**

---

# **규칙 기반/제약 기반 학습**

* **규칙 기반 학습(Regularity-based Learning)**
  > "컴퓨터가 많은 예시를 통해 **패턴이나 경향성(regularity)**을 발견하는 방식"
  * 최근접 이웃(Nearest Neighbors)
    → 이전에 봤던 거랑 제일 비슷한 걸로 판단
  * 신경망(Neural Nets)
    → 수많은 데이터로 weight를 조정하며 함수 근사
  * 부스팅(Boosting) 포함
    → 약한 모델들을 조합해 점점 정확한 분류기 만들어냄
  * 컴퓨터가 잘하는 분류/패턴 인식

  | 항목 | 설명                                                            |
  | ---- | --------------------------------------------------------------- |
  | 장점 | 대량의 데이터가 있으면 강력한 성능 (ex: 이미지 분류, 음성 인식) |
  | 단점 | 사람처럼 **추론**, **이해**, **일반화**는 잘 못함               |
  | 예시 | “고양이 사진 1,000장을 보고 고양이를 인식”                      |


* **제약 기반 학습(Constraint-based Learning)**
  > "사람처럼, 논리적 제약과 설명 가능성을 활용해 적은 정보로 학습"
  * One-shot learning → 예: “너 이거 하나 보고 외워”
  * Self-explanation → “왜 그런지 스스로 설명해보며 이해”
  * Explanation-based learning → 하나의 예제를 깊이 분석해서 일반 원리를 도출

---

## **최근접 이웃 학습 (Nearest Neighbor)**

- **구조**:
  * **특징 추출기(Feature Detector)**: 객체에서 특징 벡터 추출
  * **비교기(Comparator)**: 라이브러리와 거리 계산해 가장 가까운 것 선택
  > **예시**:
  > - 1: 콘센트 커버 분류
  > > - 특징: 총 면적, 구멍 면적
  > > - 2차원 평면에 위치시켜 가장 가까운 점 찾아 분류
  > > - **결정 경계(Decision Boundary)**: 두 점 사이 수직 이등분선으로 공간 구분
  > - 2: 의학 - 백혈구 분류
  > > - 여러 차원의 특징(핵 면적 등)을 측정해 다차원 공간에서 최근접 계산
  > - 3: 정보 검색
  > > - 문서 간 단어 빈도로 벡터 구성 (예: hack, computer)
  > > - 단어 수가 적은 probe와 관련 문서 사이의 **방향** 비교 → 더 효과적
  > > - **유사도 기준 변경**: 거리 대신 코사인 유사도 사용
  $$
  \cos \theta = \frac{u \cdot v}{\|u\| \|v\|}
  $$



---

## **로봇 팔 제어와 NN 학습**

- **고전적 접근**:
  * f = ma 사용한 동역학 방정식 → 마찰, 불확실성 등으로 **실제로 잘 작동하지 않음**

- **NN 방식 접근**:
  * 팔의 "유년기" 동안 랜덤 움직임 기록: $$\theta, \dot{\theta}, \ddot{\theta}, \text{torque}$$
  * 테이블에 저장 후, 미래의 움직임을 위해 **최근접 검색해 토크 예측**
  * 실시간 보정 가능: 점차 **인터폴레이션 정확도 향상**

- **예시**: 
  * 팔이 채찍을 휘둘러 **촛불을 끄는 동작** 학습
  * 학습 반복으로 **정확도 증가 그래프** 관찰

---

## **최근접 이웃의 한계 및 해결법**

- 문제 1: 분산 불균형 (Variance Scaling)
  * 어떤 feature(x)가 다른 feature(y)보다 값의 분포 폭이 크면 편향 생김
  * 해결법: **정규화(Normalization)**
    $$
    X' = \frac{X - \mu_X}{\sigma_X}
    $$
  * 결과: 모든 feature가 동일한 분산을 갖도록 조정

- 문제 2: 중요하지 않은 feature의 영향
  * 의미 없는 feature가 거리 계산에 끼어들어 **잡음이 유입됨**
  * 해결법: feature selection / weighting

- 문제 3: 데이터가 답을 포함하지 않을 경우
  * 예: 신용카드 회사가 파산 예측 시, 원인과 무관한 데이터만 가짐
  * ⇒ **데이터에 원인이 포함되어야 결과를 예측할 수 있음**

---

## **Diet Coke와 인과 추론**

* 개/고양이는 다이어트 콜라 마시는 사람이 모두 뚱뚱하다고 봄
* 이유: **인과(Causality)** 대신 **상관(Correlation)** 만 보고 판단하기 때문
* 결론: **인과 추론 능력 부재**로 인한 오판

---

## **✅ 정리**

* 최근접 이웃은 단순하지만 강력한 기본 학습 알고리즘
* 거리 외에도 **유사도 척도 변화(코사인 유사도 등)** 로 활용 범위 넓음
* 실제 모터 제어처럼 정밀도 요구되는 문제에도 사용 가능
