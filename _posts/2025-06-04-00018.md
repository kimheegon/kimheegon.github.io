---

title: "[MIT6.034] 18.부스팅(Boosting) 알고리즘과 약한 분류기의 결합"
date: 2025-06-05 09:30:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, Boosting, Weak-Learner, Ensemble, AdaBoost]
math: true
---

**MIT AI 강의 18 - 약한 분류기(Weak Classifier)를 강한 분류기로: 부스팅의 원리와 구현**

---

## **개요**

* 단일 분류기의 성능이 낮을 경우 ➝ **여러 개를 결합해 강한 분류기 구성**
* 핵심 개념: **부스팅(Boosting)**
* 가장 대표적인 형태: **AdaBoost (Adaptive Boosting)**

---

## **약한 분류기 (Weak Classifier)**

* 예: $h(x) \in {-1, +1}$
* 동전 던지기보다 조금 나은 성능 → **“약한” 분류기**
* 개별 분류기 오류율: $\epsilon \in (0, 0.5)$ → 무작위보다 약간 낫다면 OK

---

## **강한 분류기 (Strong Classifier)**

* 여러 개의 약한 분류기를 조합하여 예측

$$
H(x) = \text{sign}(\sum_{t} \alpha_t h_t(x))
$$

* $\alpha_t$: 분류기의 가중치 → 잘 맞춘 분류기일수록 $\alpha_t$ 큼
* 예: 다수결 혹은 가중 다수결

---

## **Decision Stump: 간단한 분류기 예시**

- 단일 조건 기반의 아주 단순한 분류기
- 예: $x_1 > c$ 이면 A 클래스, 아니면 B 클래스

$$
\text{가능한 stump 수} = 2 \times \text{cut 수} \times \text{차원 수}
$$

| 항목        | 의미                                                         |
| ----------- | ------------------------------------------------------------ |
| **cut 수**  | 각 차원(축)에서 가능한 분할 지점 수                          |
| **차원 수** | 사용할 수 있는 feature(입력 변수)의 개수                     |
| **2배**     | 같은 cut 지점이라도 **분류 방향을 반대로** 정할 수 있기 때문 |


#### **예시**

- 데이터 차원 수: 2 (예: $x_1$, $x_2$)
- 각 축마다 가능한 cut 수: 3개
- $x_1$ 축에서: cut 지점이 2, 4, 7  
- $x_2$ 축에서: cut 지점이 1, 5, 9
- 그럼 가능한 stump 개수는: $2 \times 3 \times 2 = 12$




---

## **Ensemble Learning**

**Ensemble Learning  ← 분류기 여러 개를 결합해 더 강한 분류기 생성**

├── **1. Bagging (Bootstrap Aggregating)**
│   ├─ 같은 구조의 분류기 사용 (Homogeneous)
│   ├─ 데이터 샘플링을 다르게 (with replacement)
│   ├─ 병렬 학습 (동시에 학습)
│   └─ 예: Random Forest
│
├── **2. Boosting**
│   ├─ 같은 구조의 분류기 사용 (Homogeneous)
│   ├─ 학습 순차적으로 진행
│   ├─ 이전 분류기가 틀린 샘플 → 가중치 ↑ → 다음 분류기 강조
│   ├─ 예: Adaboost, Gradient Boosting, XGBoost
│   └─ ✔ "틀린 샘플에 집중해서 점점 더 나은 분류기 구성"
│
├── **3. Stacking**
│   ├─ 서로 다른 분류기 구조도 사용 가능 (Heterogeneous)
│   ├─ 하위 분류기의 예측 결과를 모아서
│   ├─ 최종 메타 분류기가 다시 학습
│   └─ 예: SVM + Tree + NeuralNet → LogisticRegression으로 결합
│
└── **4. Voting**
    ├─ 다양한 분류기의 예측을 다수결로 결정
    ├─ Hard voting: class 다수결
    └─ Soft voting: 확률 평균 후 argmax





---

## **부스팅 알고리즘 개요**

### **목적**

* 약한 분류기 $h_t(x)$들을 순차적으로 선택해 조합

### **알고리즘 순서 (AdaBoost)**

1. **초기화**: 모든 샘플의 가중치 $w_i^{(1)} = \frac{1}{n}$
2. **반복** (for t in 1 to T):

   * (a) 현재 가중치로 가장 오류율 낮은 $h_t$ 선택
   * (b) 오류율 $\epsilon_t = \sum_{i=1}^n w_i^{(t)} \cdot [h_t(x_i) \neq y_i]$
      > - $h_t(x_i)$: 현재 약한 분류기 (stump 등)가 예측한 값
      > - $y_i$: 실제 정답
      > - $w_i^{(t)}$: t단계에서의 샘플별 가중치
      > - $[h_t(x_i) \neq y_i]$: 예측이 틀렸으면 1, 맞으면 0
      > → **틀린 샘플에 대한 전체 가중치의 합** = 현재 분류기의 오류율
   * (c) 가중치 계산:

     $$
     \alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)
     $$

    > - 분류기가 잘할수록 ${\epsilon_t}$가 작고, ${\alpha_t}$는 커짐
    > - 로그 함수를 쓰는 이유: 나중에 **가중치 업데이트가 지수 함수**로 나타나서 수학적으로 깔끔하게 정리됨
    > - 이 수식은 다음을 만족:
    >   - 틀린 분류기($\epsilon_t > 0.5$): $\alpha_t < 0$ → 영향력 감소
    >   - 완벽한 분류기($\epsilon_t = 0$): $\alpha_t \to \infty$ (이론적 극한)

   * (d) 가중치 업데이트:

     $$
     w_i^{(t+1)} = \frac{w_i^{(t)} \cdot e^{-\alpha_t y_i h_t(x_i)}}{Z_t}
     $$
     > - $y_i h_t(x_i) = \pm 1$: 맞으면 +1, 틀리면 –1
     > - 맞은 경우: $e^{-\alpha_t}$ → 가중치 감소
     > - 틀린 경우: $e^{\alpha_t}$ → 가중치 증가
     > - **즉, 틀린 샘플의 가중치를 올리고, 맞은 샘플은 줄이는 방식**
     > - $Z_t$: 정규화 상수 → 전체 가중치 합이 1 되도록 조정




3. **최종 분류기**:

$$
H(x) = \text{sign}(\sum_{t=1}^{T} \alpha_t h_t(x))
$$

---

## **주요 수학적 결과**

### **재귀적 구조**

* 각 단계에서 **이전 분류기의 실수를 보완하는 방향으로 다음 분류기 구성**
* 잘 분류된 샘플은 낮은 가중치, 틀린 샘플은 높은 가중치 부여

### **오류율 경계**

* 전체 오류율은 아래와 같은 지수 함수에 의해 **경계(bound)** 됨:

$$
\prod_t Z_t \leq e^{-2\gamma^2 T}
$$


- $Z_t$: 매 라운드의 정규화 상수 (학습 성능이 나쁠수록 커짐)
- $\gamma$: 각 분류기의 마진(최소한의 정확도 잉여): $\gamma = \frac{1}{2} - \epsilon_t$
- $T$: 반복 횟수

- 의미:
  - $\gamma > 0$인 약한 분류기가 반복될수록
  - $\prod_t Z_t$는 지수적으로 작아짐
  - 결국 전체 오류율도 **지수적으로 감소**함
  - 약한 분류기만 있어도, $\gamma > 0$만 유지하면
  - **Adaboost는 충분한 반복만으로 오류율을 0으로 수렴 가능**


---

## **최적화 없이도 가능한 가중치 업데이트**

### **통찰 (Luis Ortiz 발견)**

* 올바르게 분류한 샘플들의 전체 가중치 합 = 1/2
* 틀린 샘플들의 가중치 합도 = 1/2
* ➝ 정규화 없이 **비례적으로 스케일만 조정** 가능

> 로그, 지수, Z 계산 없이도 구현 가능 → 수험 대비에 유리
>  Z를 안구하고, 틀린/맞춘 샘플 수에 맞게 가중치의 스케일을 조정하는 것임.


---

## **실험 예시**

* 다양한 2D 분포에서 부스팅을 적용
* 몇 번의 반복만으로 모든 샘플 정확히 분류 가능
* outlier가 있는 경우에도 부스팅은 **오버피팅 방지 성능 유지**

> 각 스텀프가 오차점(outlier)을 정밀하게 감싸면서 **다른 샘플과 격리**됨

---

## **정리**

| 개념        | 요약                                             |
| ----------- | ------------------------------------------------ |
| 약한 분류기 | 성능은 낮지만 조금은 의미 있는 분류기            |
| 부스팅      | 여러 개 약한 분류기를 조합해 성능 향상           |
| AdaBoost    | 오류율 기반으로 가중치 조정하는 대표적 알고리즘  |
| 수학적 보장 | 오류율은 지수적으로 감소함 (실험적으로도 확인됨) |

> 핵심 교훈: **많은 약한 지식도 구조화하면 강력한 지능이 될 수 있다.**
