---

title: "[MIT6.034] 18.부스팅(Boosting) 알고리즘과 약한 분류기의 결합"
date: 2025-06-05 09:30:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, Boosting, Weak-Learner, Ensemble, AdaBoost]
math: true
---

**🧠 MIT AI 강의 18 - 약한 분류기(Weak Classifier)를 강한 분류기로: 부스팅의 원리와 구현**

---

## 🚀 개요

* 단일 분류기의 성능이 낮을 경우 ➝ **여러 개를 결합해 강한 분류기 구성**
* 핵심 개념: **부스팅(Boosting)**
* 가장 대표적인 형태: **AdaBoost (Adaptive Boosting)**

---

## 🧩 약한 분류기 (Weak Classifier)

* 예: $h(x) \in {-1, +1}$
* 동전 던지기보다 조금 나은 성능 → **“약한” 분류기**
* 개별 분류기 오류율: $\epsilon \in (0, 0.5)$ → 무작위보다 약간 낫다면 OK

---

## 🧠 강한 분류기 (Strong Classifier)

* 여러 개의 약한 분류기를 조합하여 예측

$$
H(x) = \text{sign}(\sum_{t} \alpha_t h_t(x))
$$

* $\alpha_t$: 분류기의 가중치 → 잘 맞춘 분류기일수록 $\alpha_t$ 큼
* 예: 다수결 혹은 가중 다수결

---

## 🌱 Decision Stump: 간단한 분류기 예시

* 단일 조건 기반 분류기 (ex. $x > c$)
* 2차원 공간에서 가능한 stump 개수: $2 \times \text{cut 수} \times \text{차원 수}$
* 분류 방향 반전 포함

---

## 🔁 부스팅 알고리즘 개요

### 🎯 목적

* 약한 분류기 $h_t(x)$들을 순차적으로 선택해 조합

### 🔄 알고리즘 순서 (AdaBoost)

1. **초기화**: 모든 샘플의 가중치 $w_i^{(1)} = \frac{1}{n}$
2. **반복** (for t in 1 to T):

   * (a) 현재 가중치로 가장 오류율 낮은 $h_t$ 선택
   * (b) 오류율 $\epsilon_t = \sum_{i=1}^n w_i^{(t)} \cdot \[h_t(x_i) \neq y_i]$
   * (c) 가중치 계산:

     $$
     \alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)
     $$
   * (d) 가중치 업데이트:

     $$
     w_i^{(t+1)} = \frac{w_i^{(t)} \cdot e^{-\alpha_t y_i h_t(x_i)}}{Z_t}
     $$
   * $Z_t$: 정규화 상수 → 전체 가중치 합이 1 되도록 조정
3. **최종 분류기**:

$$
H(x) = \text{sign}(\sum_{t=1}^{T} \alpha_t h_t(x))
$$

---

## 🎯 주요 수학적 결과

### 📌 재귀적 구조

* 각 단계에서 **이전 분류기의 실수를 보완하는 방향으로 다음 분류기 구성**
* 잘 분류된 샘플은 낮은 가중치, 틀린 샘플은 높은 가중치 부여

### 📉 오류율 경계

* 전체 오류율은 아래와 같은 지수 함수에 의해 **경계(bound)**됨:

$$
\prod_t Z_t \leq e^{-2\gamma^2 T}
$$

* $\gamma$: 분류기 마진
* $T$: 반복 횟수
* 💡 즉, **충분히 반복하면 오류율은 0에 수렴 가능**

---

## 🧮 최적화 없이도 가능한 가중치 업데이트

### 🧠 통찰 (Luis Ortiz 발견)

* 올바르게 분류한 샘플들의 전체 가중치 합 = 1/2
* 틀린 샘플들의 가중치 합도 = 1/2
* ➝ 정규화 없이 **비례적으로 스케일만 조정** 가능

> 💡 로그, 지수, Z 계산 없이도 구현 가능 → 수험 대비에 유리

---

## 🧪 실험 예시

* 다양한 2D 분포에서 부스팅을 적용
* ✔️ 몇 번의 반복만으로 모든 샘플 정확히 분류 가능
* ✔️ outlier가 있는 경우에도 부스팅은 **오버피팅 방지 성능 유지**

> 각 스텀프가 오차점(outlier)을 정밀하게 감싸면서 **다른 샘플과 격리**됨

---

## ✅ 정리

| 개념        | 요약                                             |
| ----------- | ------------------------------------------------ |
| 약한 분류기 | 성능은 낮지만 조금은 의미 있는 분류기            |
| 부스팅      | 여러 개 약한 분류기를 조합해 성능 향상           |
| AdaBoost    | 오류율 기반으로 가중치 조정하는 대표적 알고리즘  |
| 수학적 보장 | 오류율은 지수적으로 감소함 (실험적으로도 확인됨) |

> 💬 핵심 교훈: **많은 약한 지식도 구조화하면 강력한 지능이 될 수 있다.**
