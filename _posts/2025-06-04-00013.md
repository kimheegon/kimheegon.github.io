---

title: "[MIT6.034] 13.딥러닝: Convolutional Networks, Autoencoder, Dropout, Softmax"
date: 2025-06-05 00:05:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, Deep-Learning, CNN, Autoencoder, Dropout, Softmax]
math: true
---

**🧠 MIT AI 강의 13 - 딥러닝 구조와 개념의 확장: CNN, Autoencoder, Dropout**

---

## 🚀 개요

* 이번 강의는 **2개 파라미터 신경망에서 시작해, 6천만 파라미터 딥넷**까지 도약
* 핵심 주제들:

  * 계산 재사용과 효율성
  * 합성곱 신경망 (CNN)
  * 자동 부호화 (Autoencoder)
  * 소프트맥스 (Softmax)
  * 드롭아웃 (Dropout)

---

## 🔁 계산 재사용 원리 (Reuse Principle)

* **출력 노드에 영향을 주는 가중치의 변화**는 반드시 특정 경로(y 벡터 등)를 통해 전달됨
* 체인룰 기반 미분 계산에서 **중복 계산 다수 발생 → 재사용 가능**
* 슬로건: *"What's computed is computed and need not be recomputed"*
* 💡 덕분에 **계산 복잡도는 깊이에 대해 선형(linear)**

---

## 📐 합성곱 신경망 (Convolutional Neural Network)

### 🎯 기본 개념

* 이미지 입력 (예: 256x256 크기)
* 작은 영역 (예: 10x10 커널)을 이동시키며 연산 → **특징 맵(feature map)** 생성
* 커널: 하나의 뉴런이 전체 이미지에 적용되는 형태
* 여러 커널 사용 시 → 100개 이상의 특징맵 생성 가능

### 💧 풀링 (Pooling)

* 특징맵 내에서 **지역 내 최대값**만 추출 → max pooling
* **차원 축소 + 주요 정보 유지** 목적

### 📊 전체 구조 예시

* 이미지 → 다수의 커널 → max pooling → 완전 연결층 (fully connected layer) → 출력층
* **출력층은 분류 확률 반환** (예: 1000 클래스 중 하나)

---

## 🔁 오토인코딩 (Autoencoding)

### 🧪 구조

* 입력층 → 좁은 **숨겨진층(hidden layer)** → 출력층
* 목표: **출력 = 입력** 되도록 학습
* 좁은 중간층을 거치며 **입력 정보를 압축 → 일반화 추상화**

### 🧪 예시

* 키에 따른 그림자 입력 → 중간층 3개 뉴런 → 출력층 재구성
* 학습 후에도 중간 뉴런은 **직접적 의미 파악 불가 → 암호화된 일반화(Encoded Generalization)**

---

## 📈 출력층과 확률적 분류

### 🎯 시그모이드 함수와 출력층

* 출력값 $$z = \frac{1}{1 + e^{-w \cdot x + T}}$$
* $$T$$: 시그모이드 이동 (threshold)
* $$w$$: 곡선의 경사 조절

### 📊 확률 모델링

* 긍정 샘플은 시그모이드 상의 높은 확률 위치
* 부정 샘플은 $$1 - z$$로 해석
* $$w$$, $$T$$ 조정 → 주어진 샘플에 대한 **관측 확률 최대화**

---

## 🔄 소프트맥스 (Softmax)

### 📐 정의

* 여러 클래스 중 확률 분포 추정 시 사용
* 각 클래스 출력:

$$
P(c_i) = \frac{z_i}{\sum_j z_j}
$$

* 출력 벡터를 확률 분포로 변환
* top-5 예측이 가능한 이유: **확률 기반 분류**

---

## 🧬 학습 구조 확장: Pre-training + Fine-tuning

* Autoencoder로 학습된 **입력\~중간층 가중치 freeze**
* 이후 Softmax 포함한 출력층 학습 진행
* 두 구조 결합 시 **학습 성능 향상 + 학습 시간 단축** 가능

---

## 🩹 드롭아웃 (Dropout)

* 뉴런 일부를 임의로 비활성화 → **과적합 방지, local optimum 탈출**
* 각 학습 iteration마다 랜덤으로 다른 뉴런 비활성화
* 넓은 네트워크일수록 local optimum → saddle point로 변화 → 탐색 용이

---

## 🧪 실험 시연

* 5층 깊이 네트워크, 초기엔 출력값 모두 0.5
* 반복 학습 (예: 160,000회) 후 성능 급상승
* 뉴런 제거 실험:

  * 학습 후 제거: 영향 적음
  * 학습 전 제거: 성능 급감 (local maximum 진입)

---

## 🧠 딥넷의 한계

* 신경망은 **우리가 보는 방식과 다르게 시각** 처리
* 예: 픽셀 몇 개만 수정해도 전혀 다른 결과 도출
* 이미지 캡셔닝 시스템도 실제 이해 아닌 **패턴 매칭 기반**
* 결론: **기계는 추상 의미는 모르지만 통계적으로 일관된 패턴 인식은 탁월**

---

## ✅ 정리

* CNN의 핵심: **커널 적용(convolution) + 풀링(pooling) + fully connected layer**
* Autoencoder는 **데이터 일반화 학습**
* Softmax는 **확률 기반 다중 분류** 지원
* Dropout은 **local optimum 탈출 유도**
* 💡 핵심: 넓은 구조, 효율적 학습 알고리즘, 그리고 수학적 구조적 단순성이 결합되어 **딥러닝의 성능을 이끎**
