---

title: "[MIT6.034] 13.딥러닝: Convolutional Networks, Autoencoder, Dropout, Softmax"
date: 2025-06-05 00:05:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, Deep-Learning, CNN, Autoencoder, Dropout, Softmax]
math: true
---

**🧠 MIT AI 강의 13 - 딥러닝 구조와 개념의 확장: CNN, Autoencoder, Dropout**

---

## 🚀 개요

* 이번 강의는 **2개 파라미터로 구성된 신경망에서 시작해, 6천만 파라미터를 가지는 딥넷**까지 도약 과정을 살펴본다.
* 핵심 주제들:

  * 계산 재사용과 효율성
  * 합성곱 신경망 (CNN)
  * 오토 인코더 (Autoencoder)
  * 소프트맥스 (Softmax)
  * 드롭아웃 (Dropout)

---

## 🔁 계산 재사용 원리 (Reuse Principle)

* **출력 노드에 영향을 주는 가중치의 변화**는 반드시 특정 경로(y 벡터 등)를 통해 전달됨
* 체인룰 기반 미분 계산에서 **중복 계산 다수 발생 → 재사용 가능**
* 슬로건: *"What's computed is computed and need not be recomputed"*
* 💡 덕분에 **계산 복잡도는 깊이에 대해 선형(linear)**

---

## 📐 합성곱 신경망 (Convolutional Neural Network)

### 🎯 기본 개념

* 이미지 입력 (예: 256x256 크기)
* 작은 영역 (예: 10x10 커널)을 이동시키며 연산 → **특징 맵(feature map)** 생성
* 커널: 하나의 뉴런이 전체 이미지에 적용되는 형태
* 여러 커널 사용 시 → 여러 특징맵 생성 가능
  > Convolution 커널:  
      - 연산 방식: 커널(예: 10×10)이 이미지 위를 슬라이딩하면서 가중합 연산 (곱하고 더함)  
      - 목적: 특징 추출(feature extraction)  
    Max Pooling 커널:  
      - 연산 방식: pooling 윈도우(예: 2×2)가 위를 슬라이딩하면서 최댓값만 선택  
      - 목적: 공간 축소(spatial downsampling) + 가장 중요한 정보 유지

목적: 공간 축소(spatial downsampling) + 가장 중요한 정보 유지
<br>
<br>

### 💧 풀링 (Pooling)

* 특징맵 내에서 **지역 내 최대값**만 추출 → max pooling
* **차원 축소 + 주요 정보 유지** 목적


### 📊 전체 구조 예시

```
  이미지  
  ↓  
  [Conv Layer 1] 여러 커널 → 여러 개의 Feature Map  
  ↓  
  [Pooling Layer 1] 각 Feature Map에 Max Pooling → 작아진 Feature Map  
  ↓  
  (필요 시 여러 Conv+Pooling 반복)  
  ↓  
  [Flatten Layer] 모든 Feature Map을 1D 벡터로 변환  
  ↓  
  [Fully Connected Layer] → 뉴런이 전체 벡터와 연결됨  
  ↓  
  [Output Layer] → 예측 결과 출력  
```

---

## 🔁 오토인코딩 (Autoencoding)

오토인코더는 입력 데이터를 **압축하고 복원하는 비지도 학습 모델**이다.  
전체 구조는 **입력(input) → 인코더(encoder) → 잠재 공간(latent space) → 디코더(decoder) → 출력(output)**으로 구성된다.


### 🧪 구조

| 구성 요소 | 설명 |
|-----------|------|
| **인코더(Encoder)** | 입력 데이터를 **추상화(abstract)**하여 **잠재 공간(latent vector)**으로 변환하는 신경망 |
| **잠재 벡터(Latent Vector)** | 압축된 표현으로, **입력의 핵심적 정보만을 유지**한 형태 |
| **디코더(Decoder)** | 잠재 벡터를 바탕으로 **입력과 유사한 출력**을 생성하는 신경망. 이때 디코더의 **가중치(weight parameter)**는 학습된 복원 규칙을 내포함 |


### ✅ 오토인코더가 성공적으로 작동하는 이유

1. **입력값 자체를 복원 목표로 설정**하므로, 신경망은 입력의 핵심 정보를 **압축-보존-복원**하도록 학습된다.
2. **잠재 공간**이 병목 역할을 하면서 모델이 불필요한 정보를 버리고, **의미 있는 특성(feature)**만 통과시키도록 강제된다.
3. **비지도 학습**이므로 라벨 없이도 학습 가능하다.


### 🔍 오토인코더의 한계

| 항목 | 설명 |
|------|------|
| **재구성 오류** | 완전한 복원이 어렵고, 특히 고해상도 데이터일수록 손실이 발생함 |
| **일반화 부족** | 학습된 분포를 벗어난 데이터에 대해 재구성 품질이 낮음 |
| **생성 능력 제한** | 일반적인 AE는 **생성(generation)**보다는 **복원(reconstruction)**에 초점이 있음 |
| **라벨 정보 없음** | 분류 등의 목적에는 추가 supervision이 필요 |
| **보안 위험** | 잠재 벡터 탈취 시 입력 유추 가능, 디코더 구조 유출 시 **프라이버시 침해** 위험 존재 |

  
### 🔓 적대적 예제(Adversarial Example)에 대한 취약성

오토인코더에 **의도적으로 미세한 노이즈를 추가한 입력**(=적대적 예제)을 넣으면,  
모델은 이를 원본으로 인식하고 **복원 결과가 왜곡됨**.

> 예: 고양이 이미지에 적대적 노이즈를 추가하면, 오토인코더가 이를 "개" 형태로 복원할 수도 있음.

이는 오토인코더가 데이터 분포에만 의존하여 동작하고, **"진짜 의미"를 이해하지 못하기 때문**이다.


### 🔁 역방향 복원? 잠재 → 입력?

출력값을 다시 디코더에 넣어서 잠재 벡터로 "되돌리는" 역방향 연산은?
> 일반적으로는 불가능하다. 디코더는 입력값 복원이 목적이지, 그 결과에서 다시 잠재 벡터를 추론하도록 학습되지 않는다. 하지만 변형된 구조(예: invertible autoencoder, bidirectional AE, cycle-consistent loss 등)를 쓰면
"출력 → 잠재 → 입력" 형태의 순환도 가능하게 학습시킬 수 있다.

### 오토 인코더 vs 인코더-디코더

| 항목     | 오토인코더 (Autoencoder)     | 인코더-디코더 (Encoder-Decoder) |
| ------ | ----------------------- | ------------------------- |
| 입력과 출력 | **같음 또는 매우 유사함**        | **다름**                    |
| 목적     | **복원 (Reconstruction)** | **변환 (Transformation)**   |
| 예시     | 노이즈 제거, 이상 탐지           | 번역, 요약, 질문 응답             |
| 모델 구조  | 인코더 → 디코더 (대칭 구조多)      | 인코더 → 디코더 (비대칭도 많음)       |
| 예시 모델  | Variational Autoencoder | Seq2Seq, Transformer 등    |

* 오토인코더도 인코더-디코더 구조다.
* 하지만 입력과 출력이 같고 복원이 목적이면 오토인코더라고 부른다.
* 입력과 출력이 다르고 의미만 연결되어 있으면 인코더-디코더라고 한다.
* 즉, 구조는 같지만 사용 목적이 다르면 이름도 달라진다.


---

## 📈 출력층과 확률적 분류

### 기본 시그모이드 함수:

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

### 확장된 형태 (중심과 경사 조절):

$$
\sigma(x) = \frac{1}{1 + e^{-k(x - x_0)}}
$$

| 파라미터 | 의미 | 효과 |
|----------|------|------|
| $$x_0$$ | 중심 이동 | 곡선을 **좌우로 이동** (결정 임계점 변화) |
| $$k$$ | 경사 조절 | 곡선의 **민감도(steepness)** 변화, **k↑ ⇒ 계단처럼**, **k↓ ⇒ 완만하게** |

### 활용 예

- **Logistic Regression**에서 결정 경계 시각화
- **시뮬레이션**에서 soft threshold 설정
- **강화학습 정책**에서 확률 민감도 조정

### 실전에서는 쓰이나?

- 딥러닝에서는 보통 **Affine 변환(wx + b)**로 이미 자동으로 경사/이동 조절이 됨  
  ⇒ **따로 손으로 조절할 필요 없음**
- 해석 가능한 모델 또는 실험 설계에서 **직접 조절이 유용할 수 있음**
  - 해석 모델의 결정 경계 조절 (logistic regression)
    - $$x_0$$ = decision boundary
    - $$k$$ = confidence (더 큰 k → 더 확신 있는 결정)
  - 시뮬레이션/실험 설정에서 threshold 조절
    - 예: soft activation으로 조절된 on/off 시스템
  - 강화학습에서 action 확률 제어
    - 시그모이드 기반 정책에서 경사(k)를 조정하여 exploration/exploitation 조절 가능


### 📊 확률 모델링

* 긍정 샘플은 시그모이드 상의 높은 확률 위치
* 부정 샘플은 $$1 - \sigma(x)$$로 해석
* $$x_0$$, $$k$$ 조정 → 주어진 샘플에 대한 **관측 확률 최대화**

| 샘플    | 실제 라벨 $$y$$ | 모델 출력 $$\sigma(x)$$ | 손실 계산                       |
| ----- | --------- | ----------------- | --------------------------- |
| 긍정(1) | 1         | 0.95              | $$- \log(0.95)$$ (작음, 좋음)     |
| 부정(0) | 0         | 0.05              | $$- \log(1 - 0.05)$$ (작음, 좋음) |
| 반대    | 1         | 0.1               | $$- \log(0.1)$$ (큼, 나쁨)       |


---

## 🔄 소프트맥스 (Softmax)

### 📐 정의

* 여러 클래스 중 확률 분포 추정 시 사용
* 각 클래스 출력:

$$
P(c_i) = \frac{z_i}{\sum_j z_j}
$$

* 출력 벡터를 확률 분포로 변환
* top-5 예측이 가능한 이유: **확률 기반 분류**

---

## 🧬 학습 구조 확장: Pre-training + Fine-tuning

* Autoencoder로 학습된 **입력\~중간층 가중치 freeze**
* 이후 Softmax 포함한 출력층 학습 진행
* 두 구조 결합 시 **학습 성능 향상 + 학습 시간 단축** 가능
> freeze는 "신경망의 특정 계층의 가중치를 고정시켜서 학습되지 않도록 하는 것"을 말함. 즉, gradient가 계산되지 않고 weight가 그대로 유지되는 상태를 의미함.
* 왜 freeze를 사용하는가?  
  - 오토인코더는 입력 데이터를 압축하고 복원하는 방식으로 feature extractor 역할
    1. 오토인코더로 먼저 학습 → 인코더 부분은 좋은 표현(특징)을 학습함
    2. **그 인코더 부분을 고정(freeze)**하고
    3. 출력층(분류기 등)만 학습
    예시
      1. 오토인코더로 먼저 이미지 자체를 복원하도록 학습
      2. 인코더는 이미지의 형태, 구조, 질감을 추상화해서 잠재 벡터로 만든다
      3. 이 인코더는 고양이/강아지 구분에도 좋은 특징을 이미 학습했을 수 있음
      4. 그 인코더를 그대로 쓰고, 그 위에 Softmax 출력층만 붙여서 고양이/강아지 분류 학습
      --> 이렇게 하면 전체를 다시 학습할 필요 없이, 출력층만 학습해서도 꽤 괜찮은 성능이 나오는 경우가 많음
* 마지막 레이어만 학습해야 하는가?
  - 필요하다면 그 이상을 학습해도 상관없음
* 왜 일반적인 freeze 전략은 왜 인코더만 고정할까?
  - 오토인코더를 feature extractor로 쓸 때
    - 인코더는 복잡한 고차원 입력에서 **의미 있는 특징(latent representation)**을 잘 추출했을 거라고 믿음
    - 그 표현을 다른 작업(예: 분류)에 사용하고 싶음
    - 그래서 인코더는 freeze
    - 대신 그 위에 Softmax 출력층을 붙여서 분류기를 새로 학습
    - 디코더는 이 경우 필요조차 없음. 복원이 목적이 아니니까.
  - 복원이 필요하다면?
    - 디코더도 학습 대상이 된다.







---

## 🩹 드롭아웃 (Dropout)

* 뉴런 일부를 임의로 비활성화 → **과적합 방지, local optimum 탈출**
* 각 학습 iteration마다 랜덤으로 다른 뉴런 비활성화
* 넓은 네트워크일수록 local optimum → saddle point로 변화 → 탐색 용이
* 정말 도움이되는가???
  - 모델이 작거나 데이터가 적을때, 학습할 capacity가 적어지므로 성능 저하
  - 이미 정규화가 충분히 되어 있다면(L2 regularization, data augmentation 등) noise를 증가함.
    - dropout은 뉴런 일부를 확률적으로 끄는 것임. 즉 학습시 매 step마다 네트워크 구조가 달라짐.
    - 하지만 출력시에는 weight를 기대값으로 스케일링하여 전체 뉴런을 사용함.
    - 훈련 분포와 테스트 분포가 다름 -> train-test mismatch/internal covariate shift
    - mismatch로 성능이 불안하게 흔들리거나 떨어질 수 있음.
  - Dropout은 노이즈 기반 정규화, BatchNorm은 통계 기반 정규화로 같이 쓸때 상호작용이 안좋음.
    - BatchNorm은 mini-batch 안에서 평균과 분산을 규해 정규화하는 연산임.
    - 즉 각 배치마다 입력분포를 일정하게 맞춰줘 훈련 안정화를 하는 효과가 있음.
    - dropout이 들어가면 뉴런이 랜덤하게 꺼지므로 batchnorm이 계산하는 평균/표준편차가 불안정하고 일관성이 없어짐
    - 특히 테스트 시에는 dropout이 꺼지지만 batchnorm은 훈련때 계산한 통계값을 사용하게 되어 불일치가 발생하여 성능이 저하됨.
    - 즉 dropout이 입력을 흔들고 batchnorm은 흔들리는 입력을 기준삼아 학습하게됨.
  - 테스트셋이 너무 작거나 variance가 큰 경우
 * 그래서 실제 현업에서는?
 | 전략             | 이유                                           |
| -------------- | -------------------------------------------- |
| Dropout만 씀     | 소규모 모델에서 과적합 막기                              |
| BatchNorm만 씀   | 큰 모델에서 빠른 수렴과 안정성                            |
| 둘 다 쓸 경우       | 보통 Dropout은 **BatchNorm 이후**에 아주 약하게 넣거나 제거함 |
| Transformer 계열 | Dropout 사용은 하지만 위치와 비율을 매우 신중히 조절함           |


---


## 🧠 딥넷의 한계

* 신경망은 **우리가 보는 방식과 다르게 시각** 처리
* 예: 픽셀 몇 개만 수정해도 전혀 다른 결과 도출
* 이미지 캡셔닝 시스템도 실제 이해 아닌 **패턴 매칭 기반**
* 결론: **기계는 추상 의미는 모르지만 통계적으로 일관된 패턴 인식은 탁월**

---

## ✅ 정리

* CNN의 핵심: **커널 적용(convolution) + 풀링(pooling) + fully connected layer**
* Autoencoder는 **데이터 일반화 학습**
* Softmax는 **확률 기반 다중 분류** 지원
* Dropout은 **local optimum 탈출 유도**
* 💡 핵심: 넓은 구조, 효율적 학습 알고리즘, 그리고 수학적 구조적 단순성이 결합되어 **딥러닝의 성능을 이끎**
