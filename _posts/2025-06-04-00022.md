---

title: "[MIT6.034] 22.베이즈 정리, 나이브 베이즈, 구조 학습"
date: 2025-06-05 11:01:00 +0900
categories: [AI, AI-Lecture]
tags: [MIT-6.034, Bayesian, Naive-Bayes, Structure-Learning, Classification]
math: true
---

**MIT AI 강의 22 - 베이즈 정리와 나이브 베이즈 분류, 그리고 구조 학습**

---

## **복습: 베이지안 네트워크의 계산 구조**

* 복잡한 **Joint Probability Table (JPT)** 대신 **신념 네트워크(Belief Net)** 사용
* 각 노드는 **부모 노드에 조건부**, **비후손(non-descendants) 조건부 독립**
* **Chain Rule + 조건부 독립성** 조합 → **전체 결합 확률 표현 가능**

---

## **Chain Rule 재정리**

* 네트워크의 **하단 노드부터 위로 리스트화** → 의존성 없는 순서 정렬 가능

$$
P(C, D, B, T, R) = P(C|D) \cdot P(D|B, R) \cdot P(B) \cdot P(T|R) \cdot P(R)
$$

* 부모 정보만으로 각 노드 조건부 확률 계산 가능

---

## **확률 테이블 구성법: 시뮬레이션 기반 학습**

* 시뮬레이션 데이터로부터 각 조건부 확률을 추정
* 예시: Burglar, Raccoon → Dog (4개 조합)
* 각 경우에 대해 **관측 횟수** 기록

  * 예: (B=T, R=F) → D=T: 607회 / 총 1,000회 → $P(D|B,T)=0.607$
* 관측 수 증가 → **수렴하는 확률값 (Law of Large Numbers)**

---

## **모델 기반 시뮬레이션**

* 조건부 확률표로부터 **값 샘플링(coin flip)** 방식으로 시뮬레이션 가능
* 순서는 **상위 변수 → 하위 변수** 순으로 진행
* 예: B, R 먼저 결정 → D는 (B,R)에 따라 샘플링

---

## **구조 비교 시나리오**

* 두 신념 네트워크 모델을 비교

  * 하나는 실제 구조 반영
  * 다른 하나는 일부 연결 생략/추가
* **시뮬레이션 결과를 비교하여 정확한 구조 판별 가능**

---

## **나이브 베이즈 분류 (Naive Bayes Classification)**

### **베이즈 정리**

$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$

* **우도(Likelihood)** $P(B|A)$, **사전확률(Prior)** $P(A)$, **정규화 상수(Normalization)** $P(B)$

### **나이브 조건부 독립 가정**

$$
P(E_1, ..., E_n | C) = \prod_{i=1}^{n} P(E_i | C)
$$

* 여러 증거(Evidence)가 클래스(C)에 조건부 독립이라 가정

### **분류 기준**

* 클래스 C들 중에서 최대값:

$$
\text{argmax}_C \; P(C) \prod_{i=1}^{n} P(E_i | C)
$$

* Denominator $P(E_1,...,E_n)$은 **공통 항 → 무시 가능**

---

## **예제: 가짜 동전 판별**

* 두 동전:

  * $C_1$: $P(H)=0.8$
  * $C_2$: $P(H)=0.5$
* 관측값: H, T, H, ...
* 각 동전별 우도 계산 → 곱셈으로 누적 → 더 높은 확률 선택

> 💡 Law of Large Numbers로 플립 수 증가시 **정확도 증가**

---

## **예제2: 부모의 정치 성향 추론**

* 자녀의 정당 → 부모의 정당 추론
* 부모 정당 = 클래스, 자녀 각각 = 증거
* Conditional Probability 설정:

  * $P(Child=Dem | Parent=Dem)=0.8$ 등

---

## **구조 학습 (Structure Learning)**

### **모델 선택 (Model Selection)**

* 동일한 데이터에 대해 서로 다른 구조 비교
* 각 모델에 대해 전체 데이터의 로그우도 합 계산
* 더 높은 값을 갖는 모델 선택

### **탐색 기반 구조 최적화**

* 랜덤 탐색 기반으로 구조 수정 반복 (local max 탈출 위해 **random restart** 포함)
* 점수함수: **log(probability)의 합** 사용 (곱 연산의 underflow 방지)

---

## ✅ 정리

* **Bayesian 추론**은 불완전한 정보 하에서 강력한 판단 도구
* **Naive Bayes**: 빠르고 간결, 독립성 가정이 현실성 떨어질 수 있음
* **Structure Learning**: 모델간 구조 자체 비교도 가능
* 적용 분야: 의료 진단, 거짓말 탐지, 시스템 디버깅, 학습 평가 등
